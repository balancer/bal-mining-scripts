{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALTIME_ESTIMATOR = False\n",
    "WEEK = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import requests\n",
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from urllib.request import urlopen\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "week_1_start_ts = 1590969600\n",
    "week_end_timestamp = week_1_start_ts + WEEK * 7 * 24 * 60 * 60\n",
    "week_start_timestamp = week_end_timestamp - 7 * 24 * 60 * 60\n",
    "\n",
    "BAL_addresses = {\n",
    "    1: '0xba100000625a3754423978a60c9317c58a424e3d',\n",
    "    137: '0x9a71012b13ca4d3d0cdc72a177df3ef03b0e76a3',\n",
    "    42161: '0x040d1edc9569d4bab2d15287dc5a4f10f56a56b8'\n",
    "}\n",
    "networks = {\n",
    "    1: 'ethereum',\n",
    "    137: 'polygon',\n",
    "    42161: 'arbitrum'\n",
    "}\n",
    "\n",
    "CLAIM_PRECISION = 12 # leave out of results addresses that mined less than CLAIM_THRESHOLD\n",
    "CLAIM_THRESHOLD = 10**(-CLAIM_PRECISION)\n",
    "\n",
    "reports_dir = f'reports/{WEEK}'\n",
    "if not os.path.exists(reports_dir):\n",
    "    os.mkdir(reports_dir)\n",
    "def get_export_filename(network, token):\n",
    "    return f'{reports_dir}/__{network}_{token}_subgraph.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    warnings.warn('Running realtime estimator')\n",
    "    \n",
    "    from urllib.request import urlopen\n",
    "    import json\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/reports/_current.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    claims = json.loads(jsonurl.read())\n",
    "    claimable_weeks = [20+int(w) for w in claims.keys()]\n",
    "    most_recent_week = max(claimable_weeks)\n",
    "    # delete the estimates for the most recent published week, since now there's an official value available on IPFS\n",
    "    project_id = os.environ['GCP_PROJECT']\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.lp_estimates_multitoken\n",
    "        WHERE week = {most_recent_week}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    week_1_start = '01/06/2020 00:00:00 UTC'\n",
    "    week_1_start = datetime.strptime(week_1_start, '%d/%m/%Y %H:%M:%S %Z')\n",
    "    WEEK = int(1 + (datetime.utcnow() - week_1_start).days/7)  # this is what week we're actually in\n",
    "    week_end_timestamp = week_1_start_ts + WEEK * 7 * 24 * 60 * 60\n",
    "    week_start_timestamp = week_end_timestamp - 7 * 24 * 60 * 60\n",
    "    week_end_timestamp = int(datetime.utcnow().timestamp())\n",
    "    week_passed = (week_end_timestamp - week_start_timestamp)/(7*24*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get addresses that redirect\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redirect.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redirects = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redirects = json.load(open('config/redirect.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_block(network, start_timestamp):\n",
    "    \n",
    "    endpoint = {\n",
    "        1: 'https://api.thegraph.com/subgraphs/name/blocklytics/ethereum-blocks',\n",
    "        137: 'https://api.thegraph.com/subgraphs/name/matthewlilley/polygon-blocks',\n",
    "        42161: 'https://api.thegraph.com/subgraphs/name/ianlapham/arbitrum-one-blocks'\n",
    "    }\n",
    "    \n",
    "    query = '''\n",
    "        {\n",
    "          blocks(first: 1, orderBy: number, orderDirection: desc, where: { timestamp_lte: {}}) {\n",
    "            number\n",
    "          }\n",
    "        }\n",
    "    '''.replace('{','{{').replace('}','}}').replace('{{}}','{}').format(\n",
    "        start_timestamp,\n",
    "    )\n",
    "    \n",
    "    r = requests.post(endpoint[network], json = {'query': query})\n",
    "\n",
    "    try:\n",
    "        start_block = json.loads(r.content)['data']['blocks'][0]['number']\n",
    "    except:\n",
    "        raise Exception(json.loads(r.content)['errors'][0]['message'])\n",
    "    \n",
    "    return start_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pool_share_subgraph(pools_addresses, network, time_travel_block):\n",
    "    \n",
    "    endpoint = {\n",
    "        1: 'https://api.thegraph.com/subgraphs/name/mendesfabio/balancer-analytics-v2',\n",
    "        137: 'https://api.thegraph.com/subgraphs/name/mendesfabio/balancer-analytics-polygon-v2',\n",
    "        42161: 'https://api.thegraph.com/subgraphs/name/mendesfabio/balancer-analytics-arbitrum-v2'\n",
    "    }\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for skip in range(0, 6000, 1000):\n",
    "        query = '''\n",
    "            {\n",
    "              poolShares(\n",
    "                first: 1000,\n",
    "                skip: {},\n",
    "                block: { number: {} },\n",
    "                where: { pool_in: [\"{}\"], balance_gt: 0}) {\n",
    "                  user {\n",
    "                    id\n",
    "                  }\n",
    "                  pool {\n",
    "                    id\n",
    "                  }\n",
    "                  balance\n",
    "                }\n",
    "            }\n",
    "        '''.replace('{','{{').replace('}','}}').replace('{{}}','{}').format(\n",
    "            skip,\n",
    "            time_travel_block,\n",
    "            '\",\"'.join(pools_addresses)\n",
    "        )\n",
    "        \n",
    "        r = requests.post(endpoint[network], json = {'query':query})\n",
    "        \n",
    "        try:\n",
    "            p = json.loads(r.content)['data']['poolShares']\n",
    "        except:\n",
    "            raise Exception(json.loads(r.content)['errors'][0]['message'])\n",
    "        \n",
    "        df = pd.DataFrame(p)\n",
    "        dfs.append(df)\n",
    "                \n",
    "        if len(df.index) < 1000:\n",
    "            break        \n",
    "\n",
    "    BPT_supply_df = pd.concat(dfs)\n",
    "    BPT_supply_df['block'] = int(time_travel_block)\n",
    "    BPT_supply_df['userAddress'] = BPT_supply_df['user'].map(lambda x: x['id'])\n",
    "    BPT_supply_df['poolId'] = BPT_supply_df['pool'].map(lambda x: x['id'])\n",
    "    BPT_supply_df['balance'] = BPT_supply_df['balance'].astype(float)\n",
    "    BPT_supply_df['timestamp'] = int(week_start_timestamp)\n",
    "    \n",
    "    return BPT_supply_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpt_tranfers_subgraph(pools_addresses, network, start_timestamp, end_timestamp):\n",
    "\n",
    "    endpoint = {\n",
    "        1: 'https://api.thegraph.com/subgraphs/name/mendesfabio/balancer-analytics-v2',\n",
    "        137: 'https://api.thegraph.com/subgraphs/name/mendesfabio/balancer-analytics-polygon-v2',\n",
    "        42161: 'https://api.thegraph.com/subgraphs/name/mendesfabio/balancer-analytics-arbitrum-v2'\n",
    "    }\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        query = '''\n",
    "            {\n",
    "              transfers(\n",
    "                  first: 1000, \n",
    "                  orderBy: timestamp, \n",
    "                  where: { timestamp_gte: {}, timestamp_lt: {}, pool_in: [\"{}\"] }) {\n",
    "                id\n",
    "                pool {\n",
    "                  id\n",
    "                }\n",
    "                fromAddress\n",
    "                toAddress\n",
    "                amount\n",
    "                block\n",
    "                timestamp\n",
    "              } \n",
    "            }\n",
    "        '''.replace('{','{{').replace('}','}}').replace('{{}}','{}').format(\n",
    "            start_timestamp,\n",
    "            end_timestamp,\n",
    "            '\",\"'.join(pools_addresses)\n",
    "        )\n",
    "        \n",
    "        r = requests.post(endpoint[network], json = {'query':query})\n",
    "        \n",
    "        try:\n",
    "            p = json.loads(r.content)['data']['transfers']\n",
    "        except:\n",
    "            raise Exception(json.loads(r.content)['errors'][0]['message'])\n",
    "\n",
    "        df = pd.DataFrame(p)\n",
    "        dfs.append(df)\n",
    "        \n",
    "        if len(df.index) < 1000:\n",
    "            break\n",
    "            \n",
    "        start_timestamp = df['timestamp'].iloc[-1]\n",
    "\n",
    "    BPT_supply_df = pd.concat(dfs)  \n",
    "    BPT_supply_df['poolId'] = BPT_supply_df['pool'].map(lambda x: x['id'])\n",
    "    BPT_supply_df['amount'] = BPT_supply_df['amount'].astype(float)\n",
    "    BPT_supply_df['timestamp'] = BPT_supply_df['timestamp'].astype(int)\n",
    "    BPT_supply_df['block'] = BPT_supply_df['block'].astype(int)\n",
    "    BPT_supply_df = BPT_supply_df[['id', 'fromAddress', 'toAddress', 'poolId', 'block', 'timestamp', 'amount']]\n",
    "    BPT_supply_df = BPT_supply_df.drop_duplicates()\n",
    "    \n",
    "    df1 = BPT_supply_df.copy()\n",
    "    df1['userAddress'] = df1['fromAddress']\n",
    "    df1['balance'] = - df1['amount']\n",
    "    df1 = df1[['userAddress', 'poolId', 'block', 'timestamp', 'balance']]\n",
    "    \n",
    "    df2 = BPT_supply_df.copy()\n",
    "    df2['userAddress'] = df2['toAddress']\n",
    "    df2['balance'] = df2['amount']\n",
    "    df2 = df2[['userAddress', 'poolId', 'block', 'timestamp', 'balance']]\n",
    "    \n",
    "    return df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lm_data(shares, transfers):\n",
    "    \n",
    "    # merge and cumsum\n",
    "\n",
    "    df_bpts = shares.append(transfers)\n",
    "\n",
    "    df_bpts = df_bpts[df_bpts['userAddress'] != '0x0000000000000000000000000000000000000000']\n",
    "    df_bpts = df_bpts[df_bpts['userAddress'] != '0xBA12222222228d8Ba445958a75a0704d566BF2C8']\n",
    "\n",
    "    df_bpts = df_bpts.sort_values(by='block')\n",
    "\n",
    "    df_bpts = df_bpts.groupby(['block', 'timestamp', 'userAddress', 'poolId']).sum().groupby(level=[2, 3]).cumsum().reset_index()\n",
    "\n",
    "    # fill balance gaps\n",
    "\n",
    "    blocks_pools = df_bpts[['block', 'timestamp', 'poolId']].drop_duplicates()\n",
    "    lps_pools = df_bpts[['userAddress', 'poolId']].drop_duplicates()\n",
    "\n",
    "    util = pd.merge(lps_pools, blocks_pools, on='poolId', how='inner')\n",
    "\n",
    "    running_bpts = util.merge(df_bpts, on=['poolId', 'userAddress', 'block', 'timestamp'], how='left')\n",
    "\n",
    "    running_bpts[\"balance\"] = running_bpts.groupby(['userAddress', 'poolId'])['balance'].transform(lambda x: x.ffill().fillna(0))\n",
    "\n",
    "    # calc timestamp diffs\n",
    "\n",
    "    running_bpts = running_bpts.sort_values(['block'])\n",
    "    running_bpts['duration'] = abs(running_bpts.groupby(['userAddress', 'poolId'])['timestamp'].diff(-1))\n",
    "\n",
    "    running_bpts.loc[pd.isnull(running_bpts['duration']), 'duration'] = week_end_timestamp - running_bpts[pd.isnull(running_bpts['duration'])]['timestamp']\n",
    "\n",
    "    # shares and integrator\n",
    "\n",
    "    running_bpts['share'] = running_bpts['balance'] / running_bpts.groupby(['block', 'poolId'])['balance'].transform('sum')\n",
    "    running_bpts['share_integral'] = running_bpts['share'] * running_bpts['duration']\n",
    "\n",
    "    integrator = running_bpts.groupby(['poolId'])['share_integral'].sum().reset_index()\n",
    "    integrator = integrator.rename(columns={'share_integral': 'integral'})\n",
    "\n",
    "    # time-weighted shares\n",
    "\n",
    "    tw_share = running_bpts.groupby(['userAddress', 'poolId'])['share_integral'].sum().reset_index()\n",
    "\n",
    "    tw_share = tw_share.merge(integrator, on='poolId')\n",
    "\n",
    "    tw_share['tw_share'] = tw_share['share_integral'] / tw_share['integral']\n",
    "\n",
    "    tw_share = tw_share[['userAddress', 'poolId', 'tw_share']]\n",
    "    tw_share = tw_share.rename(columns={'userAddress': 'miner', 'poolId': 'pool_address'})\n",
    "    \n",
    "    return tw_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2_liquidity_mining(week, \n",
    "                        pools_addresses_and_tokens_earned,\n",
    "                        BPT_share_df,\n",
    "                        network):\n",
    "    \n",
    "    network_name = networks[network]\n",
    "    \n",
    "    BPT_share_df['miner'] = BPT_share_df['miner'].apply(Web3.toChecksumAddress)\n",
    "    BPT_share_df.set_index(['pool_address','miner'], inplace=True)\n",
    "\n",
    "    bal_mined_v2 = pools_addresses_and_tokens_earned.mul(BPT_share_df['tw_share'], axis=0)\n",
    "\n",
    "    if REALTIME_ESTIMATOR:\n",
    "        bal_mined_v2 *= week_passed\n",
    "\n",
    "    miner_export = bal_mined_v2.groupby('miner').sum()\n",
    "\n",
    "    for token in miner_export.columns:\n",
    "        miner_export_v2 = miner_export[token].dropna()\n",
    "        print(f'\\n{miner_export_v2.sum()} {token} mined on {network_name}')\n",
    "\n",
    "        v2_miners = pd.DataFrame(miner_export_v2).reset_index()\n",
    "        n = len(v2_miners['miner'][v2_miners['miner'].isin(redirects.keys())])\n",
    "        print(f'Redirect: {n} redirectors found')\n",
    "        v2_miners['miner'] = v2_miners['miner'].apply(lambda x: redirects.get(x,x))\n",
    "        miner_export_v2 = v2_miners.groupby('miner').sum()[token]\n",
    "\n",
    "        if not REALTIME_ESTIMATOR:\n",
    "            filename = get_export_filename(network_name, token)\n",
    "            (\n",
    "                miner_export_v2[miner_export_v2>=CLAIM_THRESHOLD]\n",
    "                .apply(\n",
    "                    lambda x: format(\n",
    "                        x, \n",
    "                        f'.{CLAIM_PRECISION}f'\n",
    "                    )\n",
    "                )\n",
    "                .to_json(filename, indent=4)\n",
    "            )\n",
    "    return miner_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "\n",
      "Chain: 1\n",
      "BAL to be mined on this chain: 114000.0\n",
      "\n",
      "101499.99999999936 0xba100000625a3754423978a60c9317c58a424e3d mined on ethereum\n",
      "Redirect: 5 redirectors found\n",
      "\n",
      "74999.99999999981 0x5a98fcbea516cf06857215779fd812ca3bef1b32 mined on ethereum\n",
      "Redirect: 5 redirectors found\n",
      "\n",
      "4700.0 0x81f8f0bb1cb2a06649e51913a151f0e7ef6fa321 mined on ethereum\n",
      "Redirect: 5 redirectors found\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Chain: 137\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Chain: 42161\n",
      "BAL to be mined on this chain: 6000.0\n",
      "\n",
      "6000.000000000053 0x040d1edc9569d4bab2d15287dc5a4f10f56a56b8 mined on arbitrum\n",
      "Redirect: 0 redirectors found\n"
     ]
    }
   ],
   "source": [
    "V2_LM_ALLOCATION_URL = 'https://raw.githubusercontent.com/balancer-labs/frontend-v2/master/src/lib/utils/liquidityMining/MultiTokenLiquidityMining.json'\n",
    "jsonurl = urlopen(V2_LM_ALLOCATION_URL)\n",
    "\n",
    "try:\n",
    "    V2_ALLOCATION_THIS_WEEK = json.loads(jsonurl.read())[f'week_{WEEK}']\n",
    "except KeyError:\n",
    "    V2_ALLOCATION_THIS_WEEK = {}\n",
    "\n",
    "full_export = pd.DataFrame()\n",
    "\n",
    "for chain in V2_ALLOCATION_THIS_WEEK:\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print('\\nChain: {}'.format(chain['chainId']))\n",
    "    if chain['chainId'] in [1, 42161]:\n",
    "        df = pd.DataFrame()\n",
    "        for pool,rewards in chain['pools'].items():\n",
    "            for r in rewards:\n",
    "                pool_address = pool.lower()\n",
    "                df.loc[pool_address,r['tokenAddress']] = r['amount']\n",
    "        if len(df) == 0:\n",
    "            print('No incentives for this chain')\n",
    "            continue\n",
    "        df.fillna(0, inplace=True)\n",
    "        df.index.name = 'pool_address'\n",
    "        bal_address = BAL_addresses[chain['chainId']]\n",
    "        if bal_address in df.columns:\n",
    "            bal_on_this_chain = df[bal_address].sum()\n",
    "        else:\n",
    "            bal_on_this_chain = 0\n",
    "        print('BAL to be mined on this chain: {}'.format(bal_on_this_chain))\n",
    "        \n",
    "        start_block = get_start_block(chain['chainId'], week_start_timestamp)\n",
    "        shares = get_pool_share_subgraph(df.index, chain['chainId'], start_block)\n",
    "        transfers = get_bpt_tranfers_subgraph(df.index, chain['chainId'], week_start_timestamp, week_end_timestamp)\n",
    "        lm_data = transform_lm_data(shares, transfers)\n",
    "        \n",
    "        chain_export = v2_liquidity_mining(WEEK, df, lm_data, chain['chainId'])\n",
    "        chain_export['chain_id'] = chain['chainId']\n",
    "        full_export = full_export.append(chain_export)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total BAL mined: 107499.99999999942\n"
     ]
    }
   ],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    mainnet_BAL = pd.read_json(\n",
    "        get_export_filename(networks[1], BAL_addresses[1]), \n",
    "        typ='series', \n",
    "        convert_dates=False)\n",
    "\n",
    "#     polygon_BAL = pd.read_json(\n",
    "#         get_export_filename(networks[137], BAL_addresses[137]), \n",
    "#         typ='series', \n",
    "#         convert_dates=False)\n",
    "\n",
    "    arbitrum_BAL = pd.read_json(\n",
    "        get_export_filename(networks[42161], BAL_addresses[42161]), \n",
    "        typ='series', \n",
    "        convert_dates=False)\n",
    "\n",
    "    mined_BAL = mainnet_BAL.add(arbitrum_BAL, fill_value=0) # TODO: add(arbitrum_BAL, fill_value=0)\n",
    "\n",
    "    filename = '/_totalsLiquidityMining.json'\n",
    "    (\n",
    "        mined_BAL[mined_BAL>=CLAIM_THRESHOLD]\n",
    "        .apply(\n",
    "            lambda x: format(\n",
    "                x, \n",
    "                f'.{CLAIM_PRECISION}f'\n",
    "            )\n",
    "        )\n",
    "        .to_json(reports_dir+filename, indent=4)\n",
    "    )\n",
    "    print('Total BAL mined: {}'.format(mined_BAL.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_export_bkp = full_export.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_export = (\n",
    "    full_export_bkp\n",
    "    .set_index('chain_id', append=True)\n",
    "    .melt(\n",
    "        var_name = 'token_address', \n",
    "        value_name = 'earned',\n",
    "        ignore_index=False)\n",
    "    .reset_index()\n",
    ")\n",
    "full_export.rename(columns={'miner':'address'}, inplace=True)\n",
    "full_export.set_index(['address','chain_id','token_address'], inplace=True)\n",
    "full_export.dropna(inplace=True)\n",
    "full_export['earned'] = full_export['earned'].apply(lambda x: format(x, f'.{18}f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update real time estimates in GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if REALTIME_ESTIMATOR:\n",
    "    # zero previous week's velocity\n",
    "    sql = f'''\n",
    "        UPDATE {project_id}.bal_mining_estimates.lp_estimates_multitoken\n",
    "        SET velocity = '0'\n",
    "        WHERE week = {WEEK-1}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "    \n",
    "    try:\n",
    "        sql = f'select * from bal_mining_estimates.lp_estimates_multitoken WHERE week = {WEEK}'\n",
    "        prev_estimate = pd.read_gbq(sql, \n",
    "                        project_id=os.environ['GCP_PROJECT'])\n",
    "        prev_estimate.set_index(['address','chain_id','token_address'], inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.iloc[0]['timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < week_start_timestamp:\n",
    "        #previous estimate is last week's; compute velocity between from week_start_timestamp and week_end_timestamp\n",
    "        delta_t = (week_end_timestamp - week_start_timestamp)\n",
    "        earned = full_export['earned'].astype(float)\n",
    "        full_export['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (week_end_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = full_export.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        full_export['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "\n",
    "    full_export['timestamp'] = week_end_timestamp\n",
    "    full_export['week'] = WEEK\n",
    "    full_export.reset_index(inplace=True)\n",
    "    full_export.to_gbq('bal_mining_estimates.lp_estimates_multitoken_staging', \n",
    "                       project_id=os.environ['GCP_PROJECT'], \n",
    "                       if_exists='replace')\n",
    "\n",
    "    # merge staging into prod\n",
    "    sql = '''\n",
    "    MERGE bal_mining_estimates.lp_estimates_multitoken prod\n",
    "    USING bal_mining_estimates.lp_estimates_multitoken_staging stage\n",
    "    ON prod.address = stage.address\n",
    "    AND prod.week = stage.week\n",
    "    AND prod.chain_id = stage.chain_id\n",
    "    AND prod.token_address = stage.token_address\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET \n",
    "            earned = stage.earned,\n",
    "            velocity = stage.velocity,\n",
    "            timestamp = stage.timestamp\n",
    "    WHEN NOT MATCHED BY TARGET THEN\n",
    "        INSERT (address, week, chain_id, token_address, earned, velocity, timestamp)\n",
    "        VALUES (address, week, chain_id, token_address, earned, velocity, timestamp)\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bal4gas_V1 import compute_bal_for_gas as compute_bal_for_gas_V1\n",
    "from src.bal4gas_V2 import compute_bal_for_gas as compute_bal_for_gas_V2\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    allowlist = pd.read_json(\n",
    "        f'https://raw.githubusercontent.com/balancer-labs/assets/master/generated/bal-for-gas.json', \n",
    "        orient='index').loc['homestead'].values\n",
    "    gas_allowlist = pd.Series(allowlist).str.lower().tolist()\n",
    "    gas_allowlist.append('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "\n",
    "    \n",
    "    v1 = compute_bal_for_gas_V1(week_start_timestamp, week_end_timestamp, gas_allowlist, plot=True, verbose=True)\n",
    "\n",
    "    gas_allowlist.remove('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "    gas_allowlist.append('0x0000000000000000000000000000000000000000')\n",
    "    v2 = compute_bal_for_gas_V2(week_start_timestamp, week_end_timestamp, gas_allowlist, plot=True, verbose=True)\n",
    "    \n",
    "    merge = v1.append(v2)\n",
    "\n",
    "    totals_bal4gas = merge[['address','bal_reimbursement']].groupby('address').sum()['bal_reimbursement']\n",
    "    totals_bal4gas[totals_bal4gas>=CLAIM_THRESHOLD].apply(       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_gasReimbursement.json',\n",
    "       indent=4)\n",
    "\n",
    "    # combine BAL from liquidity mining and gas reimbursements\n",
    "    totals = mainnet_BAL.add(totals_bal4gas, fill_value=0)\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totals.json',\n",
    "       indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    print('Final Check Totals BAL')\n",
    "    \n",
    "    \n",
    "    _ethereum = pd.read_json(\n",
    "        get_export_filename(networks[1], BAL_addresses[1]), \n",
    "        typ='series', \n",
    "        convert_dates=False).sum()\n",
    "\n",
    "#     _polygon = pd.read_json(\n",
    "#         get_export_filename(networks[137], BAL_addresses[137]), \n",
    "#         typ='series', \n",
    "#         convert_dates=False).sum()\n",
    "    \n",
    "    _arbitrum = pd.read_json(\n",
    "        get_export_filename(networks[42161], BAL_addresses[42161]), \n",
    "        typ='series', \n",
    "        convert_dates=False).sum()\n",
    "    \n",
    "    \n",
    "    _lm_all_networks = pd.read_json(reports_dir+'/_totalsLiquidityMining.json', orient='index').sum().values[0]\n",
    "    _claim = pd.read_json(reports_dir+'/_totals.json', orient='index').sum().values[0]\n",
    "    print(f'Liquidity Mining Ethereum: {format(_ethereum, f\".{CLAIM_PRECISION}f\")}')\n",
    "#     print(f'Liquidity Mining Polygon: {format(_polygon, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining Arbitrum: {format(_arbitrum, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining All Networks: {format(_lm_all_networks, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Gas Reimbursement week {WEEK}: {format(_claim-_ethereum, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Claims: {format(_claim, f\".{CLAIM_PRECISION}f\")}')\n",
    "    \n",
    "    # check all reports files\n",
    "    print('\\nReports totals:')\n",
    "    checks = {}\n",
    "    for f in os.listdir(reports_dir):\n",
    "        _sum = pd.read_json(reports_dir+'/'+f, orient='index').sum().values[0]\n",
    "        checks[f] = _sum\n",
    "    display(pd.DataFrame.from_dict(checks, orient='index', columns=['total']).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'reports/{WEEK}/__arbitrum_0x040d1edc9569d4bab2d15287dc5a4f10f56a56b8_subgraph.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "subgraph = pd.DataFrame({'subgraph': data})\n",
    "\n",
    "with open(f'reports/{WEEK}/__arbitrum_0x040d1edc9569d4bab2d15287dc5a4f10f56a56b8.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "bigquery = pd.DataFrame({'bigquery': data})\n",
    "\n",
    "results = subgraph.join(bigquery)\n",
    "\n",
    "results['subgraph'] = results['subgraph'].astype(float)\n",
    "results['bigquery'] = results['bigquery'].astype(float)\n",
    "\n",
    "results['diff'] = abs(results['bigquery'] - results['subgraph'])\n",
    "\n",
    "results.sort_values(by='diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'reports/{WEEK}/__ethereum_0xba100000625a3754423978a60c9317c58a424e3d_subgraph.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "subgraph = pd.DataFrame({'subgraph': data})\n",
    "\n",
    "with open(f'reports/{WEEK}/__ethereum_0xba100000625a3754423978a60c9317c58a424e3d.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "bigquery = pd.DataFrame({'bigquery': data})\n",
    "\n",
    "results = subgraph.join(bigquery)\n",
    "\n",
    "results['subgraph'] = results['subgraph'].astype(float)\n",
    "results['bigquery'] = results['bigquery'].astype(float)\n",
    "\n",
    "results['diff'] = abs(results['bigquery'] - results['subgraph'])\n",
    "\n",
    "results[results['diff'] > 0].sort_values('diff')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

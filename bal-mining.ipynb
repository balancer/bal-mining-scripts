{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALTIME_ESTIMATOR = False\n",
    "WEEK = 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider installing rusty-rlp to improve pyrlp performance with a rust based backend\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "from web3 import Web3\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "week_1_start_ts = 1590969600\n",
    "week_end_timestamp = week_1_start_ts + WEEK * 7 * 24 * 60 * 60\n",
    "week_start_timestamp = week_end_timestamp - 7 * 24 * 60 * 60\n",
    "BAL_addresses = {\n",
    "    1: '0xba100000625a3754423978a60c9317c58a424e3d',\n",
    "    137: '0x9a71012b13ca4d3d0cdc72a177df3ef03b0e76a3',\n",
    "    42161: '0x040d1edc9569d4bab2d15287dc5a4f10f56a56b8'\n",
    "}\n",
    "networks = {\n",
    "    1: 'ethereum',\n",
    "    137: 'polygon',\n",
    "    42161: 'arbitrum'\n",
    "}\n",
    "CLAIM_PRECISION = 8 # leave out of results addresses that mined less than CLAIM_THRESHOLD\n",
    "CLAIM_THRESHOLD = 10**(-CLAIM_PRECISION)\n",
    "reports_dir = f'reports/{WEEK}'\n",
    "if not os.path.exists(reports_dir):\n",
    "    os.mkdir(reports_dir)\n",
    "def get_export_filename(network, token):\n",
    "    return f'{reports_dir}/__{network}_{token}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    warnings.warn('Running realtime estimator')\n",
    "    \n",
    "#     from urllib.request import urlopen\n",
    "#     import json\n",
    "#     url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/reports/_current.json'\n",
    "#     jsonurl = urlopen(url)\n",
    "#     claims = json.loads(jsonurl.read())\n",
    "#     claimable_weeks = [20+int(w) for w in claims.keys()]\n",
    "#     most_recent_week = max(claimable_weeks)\n",
    "    # delete the estimates for the most recent published week, since now there's an official value available on IPFS\n",
    "    project_id = os.environ['GCP_PROJECT']\n",
    "#     sql = f'''\n",
    "#         DELETE FROM {project_id}.bal_mining_estimates.lp_estimates_multitoken\n",
    "#         WHERE week = {most_recent_week}\n",
    "#     '''\n",
    "#     client = bigquery.Client()\n",
    "#     query = client.query(sql)\n",
    "#     query.result()\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    week_1_start = '01/06/2020 00:00:00 UTC'\n",
    "    week_1_start = datetime.strptime(week_1_start, '%d/%m/%Y %H:%M:%S %Z')\n",
    "    WEEK = int(1 + (datetime.utcnow() - week_1_start).days/7)  # this is what week we're actually in\n",
    "    week_end_timestamp = week_1_start_ts + WEEK * 7 * 24 * 60 * 60\n",
    "    week_start_timestamp = week_end_timestamp - 7 * 24 * 60 * 60\n",
    "    week_end_timestamp = int(datetime.utcnow().timestamp())\n",
    "    week_passed = (week_end_timestamp - week_start_timestamp)/(7*24*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get addresses that redirect\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redirect.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redirects = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redirects = json.load(open('config/redirect.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpt_supply_gbq(pools_addresses,\n",
    "                   network):\n",
    "\n",
    "    network_blocks_table = {\n",
    "        1: 'bigquery-public-data.crypto_ethereum.blocks',\n",
    "        137: 'public-data-finance.crypto_polygon.blocks',\n",
    "        42161: 'nansen-datasets-prod.crypto_arbitrum.blocks'\n",
    "    }\n",
    "\n",
    "    bpt_balances_table = {\n",
    "        1: 'blockchain-etl.ethereum_balancer.view_liquidity_mining_power',\n",
    "        137: 'blockchain-etl.polygon_balancer.view_liquidity_mining_power',\n",
    "        42161: 'blockchain-etl.arbitrum_balancer.view_V2_bpt_balances'\n",
    "    }\n",
    "\n",
    "    sql = '''\n",
    "        DECLARE pool_addresses ARRAY<STRING>;\n",
    "        SET pool_addresses = [\n",
    "            '{0}'\n",
    "        ];\n",
    "\n",
    "        SELECT block_number, token_address, SUM(balance)/1e18 AS supply\n",
    "        FROM `{1}`\n",
    "        WHERE token_address IN UNNEST(pool_addresses)\n",
    "        AND address <> '0x0000000000000000000000000000000000000000'\n",
    "        AND balance > 0\n",
    "        AND block_number = (\n",
    "            SELECT MAX(number) FROM `{2}`\n",
    "            WHERE timestamp <= TIMESTAMP_SECONDS({3}))\n",
    "        GROUP BY block_number, token_address\n",
    "    '''.format(\n",
    "        '\\',\\''.join(pools_addresses),\n",
    "        bpt_balances_table[network],\n",
    "        network_blocks_table[network],\n",
    "        week_end_timestamp\n",
    "    )\n",
    "#     print(sql)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    BPT_supply_df = (\n",
    "        client.query(sql)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "    return BPT_supply_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpt_supply_subgraph(pools_addresses,\n",
    "                            time_travel_block,\n",
    "                            network):\n",
    "\n",
    "    endpoint = {\n",
    "        1: 'https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-v2',\n",
    "        137: 'https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-polygon-v2',\n",
    "        42161: 'https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-arbitrum-v2'\n",
    "    }\n",
    "\n",
    "    query = '''\n",
    "        {\n",
    "          pools(\n",
    "                block: {number: {}},\n",
    "                where:{address_in:\n",
    "              [\"{}\"]\n",
    "            }\n",
    "          ) {\n",
    "            address\n",
    "            totalShares\n",
    "          }\n",
    "        }\n",
    "    '''.replace('{','{{').replace('}','}}').replace('{{}}','{}').format(\n",
    "        time_travel_block,\n",
    "        '\",\"'.join(pools_addresses)\n",
    "    )\n",
    "    r = requests.post(endpoint[network], json = {'query':query})\n",
    "    try:\n",
    "        p = json.loads(r.content)['data']['pools']\n",
    "    except:\n",
    "        raise Exception(json.loads(r.content)['errors'][0]['message'])\n",
    "    BPT_supply_df = pd.DataFrame(p)\n",
    "    BPT_supply_df['totalShares'] = BPT_supply_df['totalShares'].astype(float)\n",
    "    return BPT_supply_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2_liquidity_mining(week, \n",
    "                        pools_addresses_and_tokens_earned,\n",
    "                        network):\n",
    "    \n",
    "    network_name = networks[network]\n",
    "\n",
    "    network_blocks_table = {\n",
    "        1: 'bigquery-public-data.crypto_ethereum.blocks',\n",
    "        137: 'public-data-finance.crypto_polygon.blocks',\n",
    "        42161: 'nansen-datasets-prod.crypto_arbitrum.blocks'\n",
    "    }\n",
    "\n",
    "    bpt_balances_table = {\n",
    "        1: 'blockchain-etl.ethereum_balancer.view_liquidity_mining_power',\n",
    "        137: 'blockchain-etl.polygon_balancer.view_liquidity_mining_power',\n",
    "        42161: 'blockchain-etl.arbitrum_balancer.view_V2_bpt_balances'\n",
    "    }\n",
    "    \n",
    "    if network == 42161:\n",
    "        sql_file = 'src/liquidity_mining_V2_arbitrum.sql'\n",
    "    else:\n",
    "        sql_file = 'src/liquidity_mining_V2.sql'\n",
    "\n",
    "    with open(sql_file,'r') as file:\n",
    "        sql = (\n",
    "            file\n",
    "            .read()\n",
    "            .format(\n",
    "                week, \n",
    "                '\\',\\''.join(pools_addresses_and_tokens_earned.index),\n",
    "                network_blocks_table[network],\n",
    "                bpt_balances_table[network]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying Bigquery for the V2 LPs...')\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    BPT_share_df = (\n",
    "        client.query(sql)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')\n",
    "    BPT_share_df['miner'] = BPT_share_df['miner'].apply(Web3.toChecksumAddress)\n",
    "    BPT_share_df.set_index(['pool_address','miner'], inplace=True)\n",
    "\n",
    "    bal_mined_v2 = pools_addresses_and_tokens_earned.mul(BPT_share_df['tw_share'], axis=0)\n",
    "\n",
    "    if REALTIME_ESTIMATOR:\n",
    "        bal_mined_v2 *= week_passed\n",
    "\n",
    "    miner_export = bal_mined_v2.groupby('miner').sum()\n",
    "\n",
    "    for token in miner_export.columns:\n",
    "        miner_export_v2 = miner_export[token].dropna()\n",
    "        print(f'\\n{miner_export_v2.sum()} {token} mined on {network_name}')\n",
    "\n",
    "        v2_miners = pd.DataFrame(miner_export_v2).reset_index()\n",
    "        n = len(v2_miners['miner'][v2_miners['miner'].isin(redirects.keys())])\n",
    "        print(f'Redirect: {n} redirectors found')\n",
    "        v2_miners['miner'] = v2_miners['miner'].apply(lambda x: redirects.get(x,x))\n",
    "        miner_export_v2 = v2_miners.groupby('miner').sum()[token]\n",
    "\n",
    "        if not REALTIME_ESTIMATOR:\n",
    "            filename = get_export_filename(network_name, token)\n",
    "            (\n",
    "                miner_export_v2[miner_export_v2>=CLAIM_THRESHOLD]\n",
    "                .apply(\n",
    "                    lambda x: format(\n",
    "                        x, \n",
    "                        f'.{CLAIM_PRECISION}f'\n",
    "                    )\n",
    "                )\n",
    "                .to_json(filename, indent=4)\n",
    "            )\n",
    "    return miner_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "\n",
      "Chain: 1\n",
      "BAL to be mined on this chain: 107080.0\n",
      "Google BigQuery sanity check - BPT supply:\n",
      "0x7b50775383d3d6f0215a8f290f2c9e2eebbeceb2 : 116519056.494\n",
      "other than that, all good\n",
      "\n",
      "2022-01-03 15:34:16 - Querying Bigquery for the V2 LPs...\n",
      "2022-01-03 15:36:20 - Done!\n",
      "\n",
      "107079.99999999999 0xba100000625a3754423978a60c9317c58a424e3d mined on ethereum\n",
      "Redirect: 6 redirectors found\n",
      "\n",
      "74999.99999999999 0x5a98fcbea516cf06857215779fd812ca3bef1b32 mined on ethereum\n",
      "Redirect: 6 redirectors found\n",
      "\n",
      "4700.0 0x81f8f0bb1cb2a06649e51913a151f0e7ef6fa321 mined on ethereum\n",
      "Redirect: 6 redirectors found\n",
      "\n",
      "374999.9999999999 0x226f7b842e0f0120b7e194d05432b3fd14773a9d mined on ethereum\n",
      "Redirect: 6 redirectors found\n",
      "\n",
      "85000.0 0x2d94aa3e47d9d5024503ca8491fce9a2fb4da198 mined on ethereum\n",
      "Redirect: 6 redirectors found\n",
      "\n",
      "30000.0 0xcfeaead4947f0705a14ec42ac3d44129e1ef3ed5 mined on ethereum\n",
      "Redirect: 6 redirectors found\n",
      "\n",
      "2000.0000000000002 0xb62132e35a6c13ee1ee0f84dc5d40bad8d815206 mined on ethereum\n",
      "Redirect: 6 redirectors found\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Chain: 137\n",
      "BAL to be mined on this chain: 25066.66\n",
      "Google BigQuery sanity check - BPT supply:\n",
      "0x06df3b2bbb68adc8b0e302443692037ed9f91b42 : 1.005\n",
      "0xce66904b68f1f070332cbc631de7ee98b650b499 : 1.002\n",
      "other than that, all good\n",
      "\n",
      "2022-01-03 15:36:38 - Querying Bigquery for the V2 LPs...\n",
      "2022-01-03 16:01:41 - Done!\n",
      "\n",
      "25066.659999999883 0x9a71012b13ca4d3d0cdc72a177df3ef03b0e76a3 mined on polygon\n",
      "Redirect: 4 redirectors found\n",
      "\n",
      "30000.000000000393 0x580a84c73811e1839f75d86d75d88cca0c241ff4 mined on polygon\n",
      "Redirect: 4 redirectors found\n",
      "\n",
      "15000.00000000003 0xF501dd45a1198C2E1b5aEF5314A68B9006D842E0 mined on polygon\n",
      "Redirect: 4 redirectors found\n",
      "\n",
      "1666666.6600000034 0xdf7837de1f2fa4631d716cf2502f8b230f1dcc32 mined on polygon\n",
      "Redirect: 4 redirectors found\n",
      "\n",
      "27500.000000000196 0x0d500b1d8e8ef31e21c99d1db9a6444d3adf1270 mined on polygon\n",
      "Redirect: 4 redirectors found\n",
      "\n",
      "55000.00000000039 0x2e1ad108ff1d8c782fcbbb89aad783ac49586756 mined on polygon\n",
      "Redirect: 4 redirectors found\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Chain: 42161\n",
      "BAL to be mined on this chain: 12920.0\n",
      "Google BigQuery sanity check - BPT supply:\n",
      "   All good\n",
      "\n",
      "2022-01-03 16:02:14 - Querying Bigquery for the V2 LPs...\n",
      "2022-01-03 16:03:28 - Done!\n",
      "\n",
      "12920.0 0x040d1edc9569d4bab2d15287dc5a4f10f56a56b8 mined on arbitrum\n",
      "Redirect: 0 redirectors found\n",
      "\n",
      "1022.9999999999999 0x965772e0e9c84b6f359c8597c891108dcf1c5b1a mined on arbitrum\n",
      "Redirect: 0 redirectors found\n",
      "\n",
      "874.9999999999998 0x4e352cf164e64adcbad318c3a1e222e9eba4ce42 mined on arbitrum\n",
      "Redirect: 0 redirectors found\n"
     ]
    }
   ],
   "source": [
    "# V2 allocation\n",
    "V2_LM_ALLOCATION_URL = 'https://raw.githubusercontent.com/balancer-labs/frontend-v2/master/src/lib/utils/liquidityMining/MultiTokenLiquidityMining.json'\n",
    "jsonurl = urlopen(V2_LM_ALLOCATION_URL)\n",
    "try:\n",
    "    V2_ALLOCATION_THIS_WEEK = json.loads(jsonurl.read())[f'week_{WEEK}']\n",
    "except KeyError:\n",
    "    V2_ALLOCATION_THIS_WEEK = {}\n",
    "full_export = pd.DataFrame()\n",
    "for chain in V2_ALLOCATION_THIS_WEEK:\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print('\\nChain: {}'.format(chain['chainId']))\n",
    "    df = pd.DataFrame()\n",
    "    for pool,rewards in chain['pools'].items():\n",
    "        for r in rewards:\n",
    "            pool_address = pool[:42].lower()\n",
    "            df.loc[pool_address,r['tokenAddress']] = r['amount']\n",
    "    if len(df) == 0:\n",
    "        print('No incentives for this chain')\n",
    "        continue\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.index.name = 'pool_address'\n",
    "    bal_address = BAL_addresses[chain['chainId']]\n",
    "    if bal_address in df.columns:\n",
    "        bal_on_this_chain = df[bal_address].sum()\n",
    "    else:\n",
    "        bal_on_this_chain = 0\n",
    "    print('BAL to be mined on this chain: {}'.format(bal_on_this_chain))\n",
    "\n",
    "    if not REALTIME_ESTIMATOR:\n",
    "        print('Google BigQuery sanity check - BPT supply:')\n",
    "        supply_gbq = get_bpt_supply_gbq(df.index, chain['chainId'])\n",
    "        supply_gbq.set_index('token_address', inplace=True)\n",
    "        supply_gbq.index.name = 'pool_address'\n",
    "        gbq_block_number = int(supply_gbq.iloc[0]['block_number'])\n",
    "        try:\n",
    "            supply_subgraph = get_bpt_supply_subgraph(df.index, gbq_block_number, chain['chainId'])\n",
    "            supply_subgraph.set_index('address', inplace=True)\n",
    "            supply_subgraph.index.name = 'pool_address'\n",
    "            all_good = True\n",
    "            for i,r in supply_subgraph.join(supply_gbq).iterrows():\n",
    "                error = (r.supply / r.totalShares)\n",
    "                if abs(error-1) > 1e-3:\n",
    "                    all_good = False\n",
    "                    print(f'{i} : {error:.3f}')\n",
    "            if all_good:\n",
    "                print('   All good\\n')\n",
    "            else:\n",
    "                print('other than that, all good\\n')\n",
    "        except Exception as e:\n",
    "            print('   Can\\'t read subgraph: ' + e.args[0])\n",
    "            \n",
    "    chain_export = v2_liquidity_mining(WEEK, df, chain['chainId'])\n",
    "    chain_export['chain_id'] = chain['chainId']\n",
    "    full_export = full_export.append(chain_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total BAL mined: 145066.66000043\n"
     ]
    }
   ],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    mainnet_BAL = pd.read_json(\n",
    "        get_export_filename(networks[1], BAL_addresses[1]), \n",
    "        typ='series', \n",
    "        convert_dates=False)\n",
    "\n",
    "    polygon_BAL = pd.read_json(\n",
    "        get_export_filename(networks[137], BAL_addresses[137]), \n",
    "        typ='series', \n",
    "        convert_dates=False)\n",
    "\n",
    "    arbitrum_BAL = pd.read_json(\n",
    "        get_export_filename(networks[42161], BAL_addresses[42161]), \n",
    "        typ='series', \n",
    "        convert_dates=False)\n",
    "\n",
    "    mined_BAL = mainnet_BAL.add(polygon_BAL, fill_value=0).add(arbitrum_BAL, fill_value=0)\n",
    "\n",
    "    filename = '/_totalsLiquidityMining.json'\n",
    "    (\n",
    "        mined_BAL[mined_BAL>=CLAIM_THRESHOLD]\n",
    "        .apply(\n",
    "            lambda x: format(\n",
    "                x, \n",
    "                f'.{CLAIM_PRECISION}f'\n",
    "            )\n",
    "        )\n",
    "        .to_json(reports_dir+filename, indent=4)\n",
    "    )\n",
    "    print('Total BAL mined: {}'.format(mined_BAL.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_export_bkp = full_export.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_export = (\n",
    "    full_export_bkp\n",
    "    .set_index('chain_id', append=True)\n",
    "    .melt(\n",
    "        var_name = 'token_address', \n",
    "        value_name = 'earned',\n",
    "        ignore_index=False)\n",
    "    .reset_index()\n",
    ")\n",
    "full_export.rename(columns={'miner':'address'}, inplace=True)\n",
    "full_export.set_index(['address','chain_id','token_address'], inplace=True)\n",
    "full_export.dropna(inplace=True)\n",
    "full_export['earned'] = full_export['earned'].apply(lambda x: format(x, f'.{18}f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update real time estimates in GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    # zero previous week's velocity\n",
    "    sql = f'''\n",
    "        UPDATE {project_id}.bal_mining_estimates.lp_estimates_multitoken\n",
    "        SET velocity = '0'\n",
    "        WHERE week = {WEEK-1}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "    \n",
    "    try:\n",
    "        sql = f'select * from bal_mining_estimates.lp_estimates_multitoken WHERE week = {WEEK}'\n",
    "        prev_estimate = pd.read_gbq(sql, \n",
    "                        project_id=os.environ['GCP_PROJECT'])\n",
    "        prev_estimate.set_index(['address','chain_id','token_address'], inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.iloc[0]['timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < week_start_timestamp:\n",
    "        #previous estimate is last week's; compute velocity between from week_start_timestamp and week_end_timestamp\n",
    "        delta_t = (week_end_timestamp - week_start_timestamp)\n",
    "        earned = full_export['earned'].astype(float)\n",
    "        full_export['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (week_end_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = full_export.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        full_export['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "\n",
    "    full_export['timestamp'] = week_end_timestamp\n",
    "    full_export['week'] = WEEK\n",
    "    full_export.reset_index(inplace=True)\n",
    "    full_export.to_gbq('bal_mining_estimates.lp_estimates_multitoken_staging', \n",
    "                       project_id=os.environ['GCP_PROJECT'], \n",
    "                       if_exists='replace')\n",
    "\n",
    "    # merge staging into prod\n",
    "    sql = '''\n",
    "    MERGE bal_mining_estimates.lp_estimates_multitoken prod\n",
    "    USING bal_mining_estimates.lp_estimates_multitoken_staging stage\n",
    "    ON prod.address = stage.address\n",
    "    AND prod.week = stage.week\n",
    "    AND prod.chain_id = stage.chain_id\n",
    "    AND prod.token_address = stage.token_address\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET \n",
    "            earned = stage.earned,\n",
    "            velocity = stage.velocity,\n",
    "            timestamp = stage.timestamp\n",
    "    WHEN NOT MATCHED BY TARGET THEN\n",
    "        INSERT (address, week, chain_id, token_address, earned, velocity, timestamp)\n",
    "        VALUES (address, week, chain_id, token_address, earned, velocity, timestamp)\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gas Reimbursement Program\n",
    "Discontinued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from src.bal4gas_V1 import compute_bal_for_gas as compute_bal_for_gas_V1\n",
    "from src.bal4gas_V2 import compute_bal_for_gas as compute_bal_for_gas_V2\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "#     BAL for Gas was discontinued\n",
    "#     # get amount spent so far\n",
    "#     # 80k BAL were allocated to the program starting week 41\n",
    "#     BUDGET = 80000\n",
    "#     spent = 0\n",
    "#     for w in (range(41,WEEK)):\n",
    "#         week_spent = pd.read_json(\n",
    "#             f'reports/{w}/_gasReimbursement.json', \n",
    "#             typ='series', \n",
    "#             convert_dates=False).sum()\n",
    "#         spent += week_spent\n",
    "    \n",
    "#     allowlist = pd.read_json(\n",
    "#         f'https://raw.githubusercontent.com/balancer-labs/assets/master/generated/bal-for-gas.json', \n",
    "#         orient='index').loc['homestead'].values\n",
    "#     gas_allowlist = pd.Series(allowlist).str.lower().tolist()\n",
    "#     gas_allowlist.append('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "\n",
    "    \n",
    "#     v1 = compute_bal_for_gas_V1(week_start_timestamp, week_end_timestamp, gas_allowlist, plot=True, verbose=True)\n",
    "\n",
    "#     gas_allowlist.remove('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "#     gas_allowlist.append('0x0000000000000000000000000000000000000000')\n",
    "#     v2 = compute_bal_for_gas_V2(week_start_timestamp, week_end_timestamp, gas_allowlist, plot=True, verbose=True)\n",
    "    \n",
    "#     merge = v1.append(v2)\n",
    "#     # take budget into account\n",
    "#     budget_left = BUDGET-spent\n",
    "#     if (merge['bal_reimbursement'].sum() > budget_left):\n",
    "#         print(f'\\nReimbursements exceed budget ({budget_left}), capping...')\n",
    "#         merge = merge.sort_values('datetime')\n",
    "#         in_budget = merge.cumsum()['bal_reimbursement'] <= budget_left\n",
    "#         merge = merge[in_budget]\n",
    "#         week_spend = merge['bal_reimbursement'].sum()\n",
    "#         print(f'Capped! {week_spend} BAL')\n",
    "\n",
    "#     totals_bal4gas = merge[['address','bal_reimbursement']].groupby('address').sum()['bal_reimbursement']\n",
    "#     totals_bal4gas[totals_bal4gas>=CLAIM_THRESHOLD].apply(\\\n",
    "#        lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_gasReimbursement.json',\n",
    "#        indent=4)\n",
    "\n",
    "    # export totals.json for backwards compatibility with any integrations that \n",
    "    # might expect to find the claimable amount there\n",
    "    totals = mainnet_BAL#.add(totals_bal4gas, fill_value=0)\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totals.json',\n",
    "       indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Check Totals BAL\n",
      "Liquidity Mining Ethereum: 107079.99999995\n",
      "Liquidity Mining Polygon: 25066.66000041\n",
      "Liquidity Mining Arbitrum: 12920.00000007\n",
      "Liquidity Mining All Networks: 145066.66000043\n",
      "Gas Reimbursement week 83: 0.00000000\n",
      "Claims: 107079.99999995\n",
      "\n",
      "Reports totals:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>__arbitrum_0x040d1edc9569d4bab2d15287dc5a4f10f56a56b8.json</th>\n",
       "      <td>12920.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__arbitrum_0x4e352cf164e64adcbad318c3a1e222e9eba4ce42.json</th>\n",
       "      <td>875.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__arbitrum_0x965772e0e9c84b6f359c8597c891108dcf1c5b1a.json</th>\n",
       "      <td>1023.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__ethereum_0x226f7b842e0f0120b7e194d05432b3fd14773a9d.json</th>\n",
       "      <td>375000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__ethereum_0x2d94aa3e47d9d5024503ca8491fce9a2fb4da198.json</th>\n",
       "      <td>85000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__ethereum_0x5a98fcbea516cf06857215779fd812ca3bef1b32.json</th>\n",
       "      <td>75000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__ethereum_0x81f8f0bb1cb2a06649e51913a151f0e7ef6fa321.json</th>\n",
       "      <td>4700.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__ethereum_0xb62132e35a6c13ee1ee0f84dc5d40bad8d815206.json</th>\n",
       "      <td>2000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__ethereum_0xba100000625a3754423978a60c9317c58a424e3d.json</th>\n",
       "      <td>107080.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__ethereum_0xcfeaead4947f0705a14ec42ac3d44129e1ef3ed5.json</th>\n",
       "      <td>30000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__polygon_0x0d500b1d8e8ef31e21c99d1db9a6444d3adf1270.json</th>\n",
       "      <td>27500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__polygon_0x2e1ad108ff1d8c782fcbbb89aad783ac49586756.json</th>\n",
       "      <td>55000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__polygon_0x580a84c73811e1839f75d86d75d88cca0c241ff4.json</th>\n",
       "      <td>30000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__polygon_0x9a71012b13ca4d3d0cdc72a177df3ef03b0e76a3.json</th>\n",
       "      <td>25066.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__polygon_0xF501dd45a1198C2E1b5aEF5314A68B9006D842E0.json</th>\n",
       "      <td>15000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__polygon_0xdf7837de1f2fa4631d716cf2502f8b230f1dcc32.json</th>\n",
       "      <td>1666666.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_totals.json</th>\n",
       "      <td>107080.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_totalsLiquidityMining.json</th>\n",
       "      <td>145066.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         total\n",
       "__arbitrum_0x040d1edc9569d4bab2d15287dc5a4f10f5...    12920.00\n",
       "__arbitrum_0x4e352cf164e64adcbad318c3a1e222e9eb...      875.00\n",
       "__arbitrum_0x965772e0e9c84b6f359c8597c891108dcf...     1023.00\n",
       "__ethereum_0x226f7b842e0f0120b7e194d05432b3fd14...   375000.00\n",
       "__ethereum_0x2d94aa3e47d9d5024503ca8491fce9a2fb...    85000.00\n",
       "__ethereum_0x5a98fcbea516cf06857215779fd812ca3b...    75000.00\n",
       "__ethereum_0x81f8f0bb1cb2a06649e51913a151f0e7ef...     4700.00\n",
       "__ethereum_0xb62132e35a6c13ee1ee0f84dc5d40bad8d...     2000.00\n",
       "__ethereum_0xba100000625a3754423978a60c9317c58a...   107080.00\n",
       "__ethereum_0xcfeaead4947f0705a14ec42ac3d44129e1...    30000.00\n",
       "__polygon_0x0d500b1d8e8ef31e21c99d1db9a6444d3ad...    27500.00\n",
       "__polygon_0x2e1ad108ff1d8c782fcbbb89aad783ac495...    55000.00\n",
       "__polygon_0x580a84c73811e1839f75d86d75d88cca0c2...    30000.00\n",
       "__polygon_0x9a71012b13ca4d3d0cdc72a177df3ef03b0...    25066.66\n",
       "__polygon_0xF501dd45a1198C2E1b5aEF5314A68B9006D...    15000.00\n",
       "__polygon_0xdf7837de1f2fa4631d716cf2502f8b230f1...  1666666.66\n",
       "_totals.json                                         107080.00\n",
       "_totalsLiquidityMining.json                          145066.66"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    print('Final Check Totals BAL')\n",
    "    \n",
    "    \n",
    "    _ethereum = pd.read_json(\n",
    "        get_export_filename(networks[1], BAL_addresses[1]), \n",
    "        typ='series', \n",
    "        convert_dates=False).sum()\n",
    "\n",
    "    _polygon = pd.read_json(\n",
    "        get_export_filename(networks[137], BAL_addresses[137]), \n",
    "        typ='series', \n",
    "        convert_dates=False).sum()\n",
    "    \n",
    "    _arbitrum = pd.read_json(\n",
    "        get_export_filename(networks[42161], BAL_addresses[42161]), \n",
    "        typ='series', \n",
    "        convert_dates=False).sum()\n",
    "    \n",
    "    \n",
    "    _lm_all_networks = pd.read_json(reports_dir+'/_totalsLiquidityMining.json', orient='index').sum().values[0]\n",
    "    _claim = pd.read_json(reports_dir+'/_totals.json', orient='index').sum().values[0]\n",
    "    print(f'Liquidity Mining Ethereum: {format(_ethereum, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining Polygon: {format(_polygon, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining Arbitrum: {format(_arbitrum, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining All Networks: {format(_lm_all_networks, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Gas Reimbursement week {WEEK}: {format(_claim-_ethereum, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Claims: {format(_claim, f\".{CLAIM_PRECISION}f\")}')\n",
    "    \n",
    "#     This was done to reduce the number of airdrop recipients.\n",
    "#     No longer necessary since the introduction of the merkle orchard\n",
    "#     # apply threshold to BAL distributed on Polygon\n",
    "#     polygon = pd.read_json(\n",
    "#             get_export_filename(networks[137], BAL_addresses[137]), \n",
    "#             typ='series', \n",
    "#             convert_dates=False)\n",
    "#     threshold = 0.001\n",
    "#     filename = reports_dir+'/_polygon_BAL_with_threshold.json'\n",
    "#     polygon[polygon>threshold].to_json(filename, indent=4)\n",
    "#     polygon = pd.read_json(filename, typ='series', convert_dates=False).sum()\n",
    "    \n",
    "    # check all reports files\n",
    "    print('\\nReports totals:')\n",
    "    checks = {}\n",
    "    for f in os.listdir(reports_dir):\n",
    "        _sum = pd.read_json(reports_dir+'/'+f, orient='index').sum().values[0]\n",
    "        checks[f] = _sum\n",
    "    display(pd.DataFrame.from_dict(checks, orient='index', columns=['total']).sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

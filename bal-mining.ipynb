{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALTIME_ESTIMATOR = False\n",
    "WEEK = 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "from web3 import Web3\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "week_1_start_ts = 1590969600\n",
    "week_end_timestamp = week_1_start_ts + WEEK * 7 * 24 * 60 * 60\n",
    "week_start_timestamp = week_end_timestamp - 7 * 24 * 60 * 60\n",
    "BAL_addresses = {\n",
    "    1: '0xba100000625a3754423978a60c9317c58a424e3d',\n",
    "    137: '0x9a71012b13ca4d3d0cdc72a177df3ef03b0e76a3'\n",
    "}\n",
    "networks = {\n",
    "    1: 'ethereum',\n",
    "    137: 'polygon'\n",
    "}\n",
    "CLAIM_PRECISION = 12 # leave out of results addresses that mined less than CLAIM_THRESHOLD\n",
    "CLAIM_THRESHOLD = 10**(-CLAIM_PRECISION)\n",
    "reports_dir = f'reports/{WEEK}'\n",
    "if not os.path.exists(reports_dir):\n",
    "    os.mkdir(reports_dir)\n",
    "def get_export_filename(network, token):\n",
    "    return f'{reports_dir}/__{network}_{token}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    warnings.warn('Running realtime estimator')\n",
    "    \n",
    "    from urllib.request import urlopen\n",
    "    import json\n",
    "    url = 'https://ipfs.fleek.co/ipns/balancer-team-bucket.storage.fleek.co/balancer-claim/snapshot'\n",
    "    jsonurl = urlopen(url)\n",
    "    claims = json.loads(jsonurl.read())\n",
    "    claimable_weeks = [20+int(w) for w in claims.keys()]\n",
    "    most_recent_week = max(claimable_weeks)\n",
    "    # delete the estimates for the most recent published week, since now there's an official value available on IPFS\n",
    "    project_id = os.environ['GCP_PROJECT']\n",
    "    sql = f'''\n",
    "        DELETE FROM {project_id}.bal_mining_estimates.lp_estimates_multitoken\n",
    "        WHERE week = {most_recent_week}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    week_1_start = '01/06/2020 00:00:00 UTC'\n",
    "    week_1_start = datetime.strptime(week_1_start, '%d/%m/%Y %H:%M:%S %Z')\n",
    "    WEEK = int(1 + (datetime.utcnow() - week_1_start).days/7)  # this is what week we're actually in\n",
    "    week_end_timestamp = week_1_start_ts + WEEK * 7 * 24 * 60 * 60\n",
    "    week_start_timestamp = week_end_timestamp - 7 * 24 * 60 * 60\n",
    "    week_end_timestamp = int(datetime.utcnow().timestamp())\n",
    "    week_passed = (week_end_timestamp - week_start_timestamp)/(7*24*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get addresses that redirect\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redirect.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redirects = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redirects = json.load(open('config/redirect.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpt_supply_gbq(pools_addresses,\n",
    "                   network):\n",
    "\n",
    "    network_blocks_table = {\n",
    "        1: 'bigquery-public-data.crypto_ethereum.blocks',\n",
    "        137: 'public-data-finance.crypto_polygon.blocks',\n",
    "    }\n",
    "\n",
    "    bpt_balances_table = {\n",
    "        1: 'blockchain-etl.ethereum_balancer.view_token_balances_subset',\n",
    "        137: 'blockchain-etl.polygon_balancer.view_bpt_balances',\n",
    "    }\n",
    "\n",
    "    sql = '''\n",
    "        DECLARE pool_addresses ARRAY<STRING>;\n",
    "        SET pool_addresses = [\n",
    "            '{0}'\n",
    "        ];\n",
    "\n",
    "        SELECT block_number, token_address, SUM(balance)/1e18 AS supply\n",
    "        FROM `{1}`\n",
    "        WHERE token_address IN UNNEST(pool_addresses)\n",
    "        AND address <> '0x0000000000000000000000000000000000000000'\n",
    "        AND balance > 0\n",
    "        AND block_number = (SELECT MAX(number) FROM `{2}`)\n",
    "        GROUP BY block_number, token_address\n",
    "    '''.format(\n",
    "        '\\',\\''.join(pools_addresses),\n",
    "        bpt_balances_table[network],\n",
    "        network_blocks_table[network]\n",
    "    )\n",
    "#     print(sql)\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    BPT_supply_df = (\n",
    "        client.query(sql)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "    return BPT_supply_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpt_supply_subgraph(pools_addresses,\n",
    "                            time_travel_block,\n",
    "                            network):\n",
    "\n",
    "    endpoint = {\n",
    "        1: 'https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-v2',\n",
    "        137: 'https://api.thegraph.com/subgraphs/name/balancer-labs/balancer-polygon-v2',\n",
    "    }\n",
    "\n",
    "    query = '''\n",
    "        {\n",
    "          pools(\n",
    "                block: {number: {}},\n",
    "                where:{address_in:\n",
    "              [\"{}\"]\n",
    "            }\n",
    "          ) {\n",
    "            address\n",
    "            totalShares\n",
    "          }\n",
    "        }\n",
    "    '''.replace('{','{{').replace('}','}}').replace('{{}}','{}').format(\n",
    "        time_travel_block,\n",
    "        '\",\"'.join(pools_addresses)\n",
    "    )\n",
    "    r = requests.post(endpoint[network], json = {'query':query})\n",
    "    p = json.loads(r.content)['data']['pools']\n",
    "    BPT_supply_df = pd.DataFrame(p)\n",
    "    BPT_supply_df['totalShares'] = BPT_supply_df['totalShares'].astype(float)\n",
    "    return BPT_supply_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2_liquidity_mining(week, \n",
    "                        pools_addresses_and_tokens_earned,\n",
    "                        network):\n",
    "    \n",
    "    network_name = networks[network]\n",
    "\n",
    "    network_blocks_table = {\n",
    "        1: 'bigquery-public-data.crypto_ethereum.blocks',\n",
    "        137: 'public-data-finance.crypto_polygon.blocks',\n",
    "    }\n",
    "\n",
    "    bpt_balances_table = {\n",
    "        1: 'blockchain-etl.ethereum_balancer.view_token_balances_subset',\n",
    "        137: 'blockchain-etl.polygon_balancer.view_bpt_balances',\n",
    "    }\n",
    "\n",
    "    with open('src/liquidity_mining_V2.sql','r') as file:\n",
    "        sql = (\n",
    "            file\n",
    "            .read()\n",
    "            .format(\n",
    "                week, \n",
    "                '\\',\\''.join(pools_addresses_and_tokens_earned.index),\n",
    "                network_blocks_table[network],\n",
    "                bpt_balances_table[network]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying Bigquery for the V2 LPs...')\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    BPT_share_df = (\n",
    "        client.query(sql)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')\n",
    "    BPT_share_df['miner'] = BPT_share_df['miner'].apply(Web3.toChecksumAddress)\n",
    "    BPT_share_df.set_index(['pool_address','miner'], inplace=True)\n",
    "\n",
    "    bal_mined_v2 = pools_addresses_and_tokens_earned.mul(BPT_share_df['tw_share'], axis=0)\n",
    "\n",
    "    if REALTIME_ESTIMATOR:\n",
    "        bal_mined_v2 *= week_passed\n",
    "\n",
    "    miner_export = bal_mined_v2.groupby('miner').sum()\n",
    "\n",
    "    for token in miner_export.columns:\n",
    "        miner_export_v2 = miner_export[token].dropna()\n",
    "        print(f'\\n{miner_export_v2.sum()} {token} mined on {network_name}')\n",
    "\n",
    "        v2_miners = pd.DataFrame(miner_export_v2).reset_index()\n",
    "        n = len(v2_miners['miner'][v2_miners['miner'].isin(redirects.keys())])\n",
    "        print(f'Redirect: {n} redirectors found')\n",
    "        v2_miners['miner'] = v2_miners['miner'].apply(lambda x: redirects.get(x,x))\n",
    "        miner_export_v2 = v2_miners.groupby('miner').sum()[token]\n",
    "\n",
    "        if not REALTIME_ESTIMATOR:\n",
    "            filename = get_export_filename(network_name, token)\n",
    "            (\n",
    "                miner_export_v2[miner_export_v2>=CLAIM_THRESHOLD]\n",
    "                .apply(\n",
    "                    lambda x: format(\n",
    "                        x, \n",
    "                        f'.{CLAIM_PRECISION}f'\n",
    "                    )\n",
    "                )\n",
    "                .to_json(filename, indent=4)\n",
    "            )\n",
    "    return miner_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# V2 allocation\n",
    "V2_LM_ALLOCATION_URL = 'https://raw.githubusercontent.com/balancer-labs/frontend-v2/master/src/lib/utils/liquidityMining/MultiTokenLiquidityMining.json'\n",
    "jsonurl = urlopen(V2_LM_ALLOCATION_URL)\n",
    "try:\n",
    "    V2_ALLOCATION_THIS_WEEK = json.loads(jsonurl.read())[f'week_{WEEK}']\n",
    "except KeyError:\n",
    "    V2_ALLOCATION_THIS_WEEK = {}\n",
    "full_export = pd.DataFrame()\n",
    "for chain in V2_ALLOCATION_THIS_WEEK:\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print('\\nChain: {}'.format(chain['chainId']))\n",
    "    df = pd.DataFrame()\n",
    "    for pool,rewards in chain['pools'].items():\n",
    "        for r in rewards:\n",
    "            pool_address = pool[:42].lower()\n",
    "            df.loc[pool_address,r['tokenAddress']] = r['amount']\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.index.name = 'pool_address'\n",
    "    print('BAL to be mined on this chain: {}'.format(df[BAL_addresses[chain['chainId']]].sum()))\n",
    "\n",
    "    if not REALTIME_ESTIMATOR:\n",
    "        print('Google BigQuery sanity check - BPT supply:')\n",
    "        supply_gbq = get_bpt_supply_gbq(df.index, chain['chainId'])\n",
    "        supply_gbq.set_index('token_address', inplace=True)\n",
    "        supply_gbq.index.name = 'pool_address'\n",
    "        gbq_block_number = int(supply_gbq.iloc[0]['block_number'])\n",
    "        supply_subgraph = get_bpt_supply_subgraph(df.index, gbq_block_number, chain['chainId'])\n",
    "        supply_subgraph.set_index('address', inplace=True)\n",
    "        supply_subgraph.index.name = 'pool_address'\n",
    "        all_good = True\n",
    "        for i,r in supply_subgraph.join(supply_gbq).iterrows():\n",
    "            error = (r.supply / r.totalShares)\n",
    "            if abs(error-1) > 1e-3:\n",
    "                all_good = False\n",
    "                print(f'{i} : {error:.3f}')\n",
    "        if all_good:\n",
    "            print('   All good\\n')\n",
    "        else:\n",
    "            print('other than that, all good\\n')    \n",
    "            \n",
    "    chain_export = v2_liquidity_mining(WEEK, df, chain['chainId'])\n",
    "    chain_export['chain_id'] = chain['chainId']\n",
    "    full_export = full_export.append(chain_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    mainnet_BAL = pd.read_json(\n",
    "        get_export_filename(networks[1], BAL_addresses[1]), \n",
    "        typ='series', \n",
    "        convert_dates=False)\n",
    "\n",
    "    polygon_BAL = pd.read_json(\n",
    "        get_export_filename(networks[137], BAL_addresses[137]), \n",
    "        typ='series', \n",
    "        convert_dates=False)\n",
    "\n",
    "    mined_BAL = mainnet_BAL.add(polygon_BAL, fill_value=0)\n",
    "\n",
    "    filename = '/_totalsLiquidityMining.json'\n",
    "    (\n",
    "        mined_BAL[mined_BAL>=CLAIM_THRESHOLD]\n",
    "        .apply(\n",
    "            lambda x: format(\n",
    "                x, \n",
    "                f'.{CLAIM_PRECISION}f'\n",
    "            )\n",
    "        )\n",
    "        .to_json(reports_dir+filename, indent=4)\n",
    "    )\n",
    "    print('Total BAL mined: {}'.format(mined_BAL.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_export_bkp = full_export.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_export = (\n",
    "    full_export_bkp\n",
    "    .set_index('chain_id', append=True)\n",
    "    .melt(\n",
    "        var_name = 'token_address', \n",
    "        value_name = 'earned',\n",
    "        ignore_index=False)\n",
    "    .reset_index()\n",
    ")\n",
    "full_export.rename(columns={'miner':'address'}, inplace=True)\n",
    "full_export.set_index(['address','chain_id','token_address'], inplace=True)\n",
    "full_export.dropna(inplace=True)\n",
    "full_export['earned'] = full_export['earned'].apply(lambda x: format(x, f'.{18}f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update real time estimates in GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    # zero previous week's velocity\n",
    "    sql = f'''\n",
    "        UPDATE {project_id}.bal_mining_estimates.lp_estimates_multitoken\n",
    "        SET velocity = '0'\n",
    "        WHERE week = {WEEK-1}\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result();\n",
    "    \n",
    "    try:\n",
    "        sql = f'select * from bal_mining_estimates.lp_estimates_multitoken WHERE week = {WEEK}'\n",
    "        prev_estimate = pd.read_gbq(sql, \n",
    "                        project_id=os.environ['GCP_PROJECT'])\n",
    "        prev_estimate.set_index(['address','chain_id','token_address'], inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.iloc[0]['timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < week_start_timestamp:\n",
    "        #previous estimate is last week's; compute velocity between from week_start_timestamp and week_end_timestamp\n",
    "        delta_t = (week_end_timestamp - week_start_timestamp)\n",
    "        earned = full_export['earned'].astype(float)\n",
    "        full_export['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (week_end_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = full_export.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        full_export['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "\n",
    "    full_export['timestamp'] = week_end_timestamp\n",
    "    full_export['week'] = WEEK\n",
    "    full_export.reset_index(inplace=True)\n",
    "    full_export.to_gbq('bal_mining_estimates.lp_estimates_multitoken_staging', \n",
    "                       project_id=os.environ['GCP_PROJECT'], \n",
    "                       if_exists='replace')\n",
    "\n",
    "    # merge staging into prod\n",
    "    sql = '''\n",
    "    MERGE bal_mining_estimates.lp_estimates_multitoken prod\n",
    "    USING bal_mining_estimates.lp_estimates_multitoken_staging stage\n",
    "    ON prod.address = stage.address\n",
    "    AND prod.week = stage.week\n",
    "    AND prod.chain_id = stage.chain_id\n",
    "    AND prod.token_address = stage.token_address\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET \n",
    "            earned = stage.earned,\n",
    "            velocity = stage.velocity,\n",
    "            timestamp = stage.timestamp\n",
    "    WHEN NOT MATCHED BY TARGET THEN\n",
    "        INSERT (address, week, chain_id, token_address, earned, velocity, timestamp)\n",
    "        VALUES (address, week, chain_id, token_address, earned, velocity, timestamp)\n",
    "    '''\n",
    "    client = bigquery.Client()\n",
    "    query = client.query(sql)\n",
    "    query.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gas Reimbursement Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.bal4gas_V1 import compute_bal_for_gas as compute_bal_for_gas_V1\n",
    "from src.bal4gas_V2 import compute_bal_for_gas as compute_bal_for_gas_V2\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    allowlist_part1 = pd.read_json(f'https://raw.githubusercontent.com/balancer-labs/assets/master/lists/eligible.json').index.values\n",
    "    allowlist_part2 = pd.read_json(\n",
    "        f'https://raw.githubusercontent.com/balancer-labs/assets/master/lists/ui-not-eligible.json', \n",
    "        orient='index').loc['homestead'].values\n",
    "    allowlist = allowlist_part1.tolist() + allowlist_part2.tolist()\n",
    "    gas_allowlist = pd.Series(allowlist).str.lower().tolist()\n",
    "    gas_allowlist.append('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "\n",
    "    \n",
    "    v1 = compute_bal_for_gas_V1(week_start_timestamp, week_end_timestamp, gas_allowlist, plot=True, verbose=True)\n",
    "\n",
    "    gas_allowlist.remove('0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee')\n",
    "    gas_allowlist.append('0x0000000000000000000000000000000000000000')\n",
    "    v2 = compute_bal_for_gas_V2(week_start_timestamp, week_end_timestamp, gas_allowlist, plot=True, verbose=True)\n",
    "    \n",
    "    merge = v1.append(v2)\n",
    "\n",
    "    totals_bal4gas = merge[['address','bal_reimbursement']].groupby('address').sum()['bal_reimbursement']\n",
    "    totals_bal4gas[totals_bal4gas>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_gasReimbursement.json',\n",
    "       indent=4)\n",
    "\n",
    "    # combine BAL from liquidity mining and gas reimbursements\n",
    "    totals = mainnet_BAL.add(totals_bal4gas, fill_value=0)\n",
    "    totals[totals>=CLAIM_THRESHOLD].apply(\\\n",
    "       lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totals.json',\n",
    "       indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REALTIME_ESTIMATOR:\n",
    "    print('Final Check Totals BAL')\n",
    "    \n",
    "    \n",
    "    _ethereum = pd.read_json(\n",
    "        get_export_filename(networks[1], BAL_addresses[1]), \n",
    "        typ='series', \n",
    "        convert_dates=False).sum()\n",
    "\n",
    "    _polygon = pd.read_json(\n",
    "        get_export_filename(networks[137], BAL_addresses[137]), \n",
    "        typ='series', \n",
    "        convert_dates=False).sum()\n",
    "    \n",
    "    \n",
    "    _lm_both = pd.read_json(reports_dir+'/_totalsLiquidityMining.json', orient='index').sum().values[0]\n",
    "    _claim = pd.read_json(reports_dir+'/_totals.json', orient='index').sum().values[0]\n",
    "    print(f'Liquidity Mining Ethereum: {format(_ethereum, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining Polygon: {format(_polygon, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Liquidity Mining Both: {format(_lm_both, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Gas Reimbursement week {WEEK}: {format(_claim-_ethereum, f\".{CLAIM_PRECISION}f\")}')\n",
    "    print(f'Claims: {format(_claim, f\".{CLAIM_PRECISION}f\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# Google BigQuery SQL to get the blocks mined around a timestamp\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# SELECT * FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "# WHERE timestamp > \"2020-12-13 23:59:50\"\n",
    "# and timestamp < \"2020-12-14 00:00:10\"\n",
    "# order by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the window of blocks\n",
    "WEEK = 28\n",
    "START_BLOCK = 11402291\n",
    "END_BLOCK = 11447731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "# from google.cloud import bigquery_storage\n",
    "\n",
    "REALTIME_ESTIMATOR = False\n",
    "\n",
    "if REALTIME_ESTIMATOR:\n",
    "    import warnings\n",
    "    warnings.warn('Running realtime estimator')\n",
    "    WEEK = 0\n",
    "    \n",
    "    sql = '''\n",
    "    SELECT number FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "    where timestamp >= TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), WEEK(MONDAY))\n",
    "    and timestamp <= TIMESTAMP_ADD(TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), WEEK(MONDAY)), interval 1 HOUR)\n",
    "    order by timestamp\n",
    "    limit 1\n",
    "    '''\n",
    "    results = bigquery.Client().query(sql).result()\n",
    "    for row in results:\n",
    "        START_BLOCK = row.number\n",
    "\n",
    "    sql = '''\n",
    "    SELECT MAX(number) as number FROM `bigquery-public-data.crypto_ethereum.blocks`\n",
    "    '''\n",
    "    results = bigquery.Client().query(sql).result()\n",
    "    for row in results:\n",
    "        END_BLOCK = row.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-14 11:06:56\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider installing rusty-rlp to improve pyrlp performance with a rust based backend\n"
     ]
    }
   ],
   "source": [
    "from web3 import Web3\n",
    "web3_provider = os.environ['ENDPOINT_URL']\n",
    "w3 = Web3(Web3.WebsocketProvider(web3_provider))\n",
    "start_block_timestamp = w3.eth.getBlock(START_BLOCK).timestamp\n",
    "end_block_timestamp = w3.eth.getBlock(END_BLOCK).timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "28",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dd61bd89401e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwhitelist_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/balancer-labs/assets/master/lists/eligible.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mwhitelist_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mELIGIBLE_TOKENS_URL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWEEK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 28"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# get list of tokens eligible for liquidity mining\n",
    "ELIGIBLE_TOKENS_URL = {\n",
    "    24: 'https://raw.githubusercontent.com/balancer-labs/assets/6b1e9875e2fb81965be0251c359c1fc390af0e43/lists/eligible.json',\n",
    "    25: 'https://raw.githubusercontent.com/balancer-labs/assets/026076f7ef6094107a54468593ec91b9f94accc1/lists/eligible.json',\n",
    "    26: 'https://raw.githubusercontent.com/balancer-labs/assets/5c00bfadc1a128be6562bc7a758c0bc323b4a7b9/lists/eligible.json',\n",
    "    27: 'https://raw.githubusercontent.com/balancer-labs/assets/f822ccf/lists/eligible.json',\n",
    "}\n",
    "if REALTIME_ESTIMATOR:\n",
    "    whitelist_df = pd.read_json('https://raw.githubusercontent.com/balancer-labs/assets/master/lists/eligible.json')\n",
    "else:\n",
    "    whitelist_df = pd.read_json(ELIGIBLE_TOKENS_URL[WEEK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json5 as json\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/blacklisted_sharelholders.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    BLACKLISTED_SHAREHOLDERS = json.loads(jsonurl.read())['address']\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/equivalent_sets.json5'\n",
    "    jsonurl = urlopen(url)\n",
    "    EQUIVALENT_SETS = json.loads(jsonurl.read())['sets']\n",
    "else:\n",
    "    BLACKLISTED_SHAREHOLDERS = json.load(open('config/blacklisted_sharelholders.json'))['address']\n",
    "    EQUIVALENT_SETS = json.load(open('config/equivalent_sets.json5'))['sets']\n",
    "    \n",
    "BLACKLISTED_SHAREHOLDERS_lower = [x.lower() for x in BLACKLISTED_SHAREHOLDERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAL_TOKEN = '0xba100000625a3754423978a60c9317c58a424e3D'\n",
    "SNAPSHOT_WINDOW_SIZE = 256\n",
    "CLAIM_PRECISION = 18 # leave out of results addresses that mined less than 1E-18 BAL\n",
    "WEEKLY_MINED = 145000\n",
    "LIQUIDITY_STAKING = 45000\n",
    "if REALTIME_ESTIMATOR:\n",
    "    week_passed = (end_block_timestamp - start_block_timestamp)/(7*24*3600)\n",
    "    WEEKLY_MINED = int(WEEKLY_MINED*week_passed)\n",
    "    LIQUIDITY_STAKING = int(LIQUIDITY_STAKING*week_passed)\n",
    "STAKERS_SHARE = LIQUIDITY_STAKING / WEEKLY_MINED\n",
    "reports_dir = f'reports/{WEEK}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(reports_dir):\n",
    "    os.mkdir(reports_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "flatten_list = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "def get_wrap_factor_of_pair(a, b):\n",
    "    a = Web3.toChecksumAddress(a)\n",
    "    b = Web3.toChecksumAddress(b)\n",
    "    for soft in EQUIVALENT_SETS:\n",
    "        for hard in soft:\n",
    "            if a in hard and b in hard:\n",
    "                return .1\n",
    "        flat = flatten_list(soft)\n",
    "        if a in flat and b in flat:\n",
    "            return .2\n",
    "    return 1\n",
    "\n",
    "\n",
    "def get_wrap_factor(tokens_weights):\n",
    "    length = len(tokens_weights)\n",
    "    shape = (length, length)\n",
    "    wf_matrix = np.zeros(shape)\n",
    "    tokens = tokens_weights.index.get_level_values('token_address')\n",
    "    for i in range(len(tokens_weights)-1):\n",
    "        for j in range(i+1, len(tokens_weights)):\n",
    "            wf_matrix[i,j] = wf_matrix[i,j] + get_wrap_factor_of_pair(tokens[i],tokens[j])\n",
    "    weights_vector = np.array(tokens_weights)\n",
    "    weights_matrix = np.outer(weights_vector,weights_vector.T)\n",
    "    try:\n",
    "        element_wise_product = weights_matrix * wf_matrix\n",
    "    except:\n",
    "        print(tokens_weights)\n",
    "        print(weights_matrix)\n",
    "        print(wf_matrix)\n",
    "        raise\n",
    "    return element_wise_product.sum()/np.triu(weights_matrix, k=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_staking_boosts_of_pair(a, b, bal_multiplier=1):\n",
    "    a = Web3.toChecksumAddress(a)\n",
    "    b = Web3.toChecksumAddress(b)\n",
    "    if a==BAL_TOKEN:\n",
    "        cap_b = whitelist_df.loc[b.lower(),'cap']\n",
    "        if cap_b == np.inf:\n",
    "            return (bal_multiplier, 1)\n",
    "        else:\n",
    "            return (1, 1)\n",
    "    elif b==BAL_TOKEN:\n",
    "        cap_a = whitelist_df.loc[a.lower(),'cap']\n",
    "        if cap_a == np.inf:\n",
    "            return (1, bal_multiplier)\n",
    "        else:\n",
    "            return (1, 1)\n",
    "    else:\n",
    "        return (1, 1)\n",
    "\n",
    "\n",
    "def get_BRF(tokens_weights, bal_multiplier=1):\n",
    "    if type(bal_multiplier) != int: # expect a series of BAL multipliers on the third pass\n",
    "        block_number = tokens_weights.index.get_level_values('block_number').drop_duplicates().values\n",
    "        if len(block_number)>1:\n",
    "            raise Exception('got more than one block_number {}'.format(block_number))\n",
    "        else:\n",
    "            bal_multiplier = final_bal_multiplier[block_number[0]]\n",
    "            \n",
    "    if 'shareholders_subpool' in tokens_weights.index.names:\n",
    "        if tokens_weights.index.get_level_values('shareholders_subpool').all():\n",
    "            bal_multiplier=1\n",
    "    else:\n",
    "        if tokens_weights.index.nlevels==4:\n",
    "            raise Exception('shareholders_subpool not in index')    \n",
    "            \n",
    "    denominator = 0\n",
    "    numerator = 0\n",
    "    token_address = list(tokens_weights.index.get_level_values('token_address'))\n",
    "    token_weights = list(tokens_weights)\n",
    "    for i in range(len(tokens_weights)-1):\n",
    "        for j in range(i+1, len(tokens_weights)):\n",
    "            token_A = token_address[i]\n",
    "            token_B = token_address[j]\n",
    "            weight_A = token_weights[i]\n",
    "            weight_B = token_weights[j]\n",
    "            staking_boosts = get_staking_boosts_of_pair(token_A, token_B, bal_multiplier)\n",
    "            staking_boost_A = staking_boosts[0]\n",
    "            staking_boost_B = staking_boosts[1]\n",
    "            staking_boost_of_pair = (staking_boost_A * weight_A + staking_boost_B * weight_B) / (weight_A + weight_B)\n",
    "            \n",
    "            ratio_factor = 4 * (weight_A / (weight_A + weight_B)) * (weight_B / (weight_A + weight_B))\n",
    "            \n",
    "            numerator += staking_boost_of_pair * ratio_factor * weight_A * weight_B\n",
    "            denominator += weight_A * weight_B\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def get_list_of_snapshot_blocks(start, end):\n",
    "    block_list = range(end, start, -SNAPSHOT_WINDOW_SIZE)\n",
    "    block_list = list(block_list)\n",
    "    block_list.sort()\n",
    "    return block_list\n",
    "\n",
    "snapshot_blocks = get_list_of_snapshot_blocks(START_BLOCK, END_BLOCK)\n",
    "snapshot_timestamps_blocks = {w3.eth.getBlock(b).timestamp: b \\\n",
    "                              for b in tqdm(snapshot_blocks, 'Getting snapshot timestamps')}\n",
    "snapshot_blocks_timestamps = {v: k for k,v in snapshot_timestamps_blocks.items()}\n",
    "snapshot_blocks_as_str = [str(b) for b in snapshot_blocks]\n",
    "print('week {}: {} snapshot blocks'.format(WEEK, len(snapshot_blocks)))\n",
    "print('week {}: first snapshot block: {} ({}...)'.format(WEEK, min(snapshot_blocks), snapshot_blocks[:3]))\n",
    "print('week {}: last snapshot block: {} (...{})'.format(WEEK, max(snapshot_blocks), snapshot_blocks[-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove kovan\n",
    "whitelist_df.drop(columns=['kovan'], inplace=True)\n",
    "whitelist_df.dropna(inplace=True)\n",
    "# convert string to cap\n",
    "whitelist_df.rename(columns={'homestead':'cap'}, inplace=True)\n",
    "CAP_TIERS = {\n",
    "    'cap1':   1e6,\n",
    "    'cap2':   3e6,\n",
    "    'cap3':  10e6,\n",
    "    'cap4':  30e6,\n",
    "    'cap5': 100e6,\n",
    "    'uncapped': np.inf,\n",
    "}\n",
    "whitelist_df['cap'] = whitelist_df['cap'].apply(lambda x: CAP_TIERS[x])\n",
    "# lower case the token addresses for later join\n",
    "whitelist_df.index.name = 'checksum_token_address'\n",
    "whitelist_df.reset_index(inplace=True)\n",
    "whitelist_df['checksum_token_address'] = whitelist_df['checksum_token_address'].apply(Web3.toChecksumAddress)\n",
    "whitelist_df.set_index(whitelist_df['checksum_token_address'].str.lower(), inplace=True)\n",
    "whitelist_df.index.name = 'token_address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get decimals of whitelisted tokens\n",
    "import json\n",
    "from web3.exceptions import ABIFunctionNotFound, BadFunctionCallOutput, InvalidAddress\n",
    "token_abi = json.load(open('abi/BToken.json'))\n",
    "def get_token_decimals(token_address):\n",
    "    try:\n",
    "        token_contract = w3.eth.contract(token_address, abi=token_abi)\n",
    "    except InvalidAddress:\n",
    "        print('Invalid Address: ' + token_address)\n",
    "        raise\n",
    "    try:\n",
    "        return token_contract.functions.decimals().call()\n",
    "    except ABIFunctionNotFound:\n",
    "        print(f'{token_address} does not implement decimals(), assuming 0')\n",
    "        return 0\n",
    "    except BadFunctionCallOutput:\n",
    "        print(f'{token_address}: decimals() returned bad output, assuming 0')\n",
    "        return 0\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Getting token decimals from ethereum node...')\n",
    "whitelist_df['decimals'] = whitelist_df['checksum_token_address'].apply(get_token_decimals)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get USD prices of whitelist tokens\n",
    "import requests\n",
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "try:\n",
    "    prices_json = json.load(open(reports_dir+'/_prices.json'))\n",
    "except FileNotFoundError:\n",
    "    prices_json = {}\n",
    "MARKET_API_URL = 'https://api.coingecko.com/api/v3'\n",
    "price_query = MARKET_API_URL+'/coins/ethereum/contract/{}/market_chart/range?&vs_currency=usd&from={}&to={}'\n",
    "time_bounded_price_query = price_query.format('{}', start_block_timestamp, end_block_timestamp)\n",
    "whitelist_df['prices_api_response'] = ''\n",
    "whitelist_df['prices_dict'] = ''\n",
    "for i in tqdm(whitelist_df.index, 'Getting prices'):\n",
    "    checksum_token_address = whitelist_df.loc[i,'checksum_token_address']\n",
    "    query_url = time_bounded_price_query.format(checksum_token_address)\n",
    "    # when running week 26 we ran across a bug in Coingecko price feed where the price of WETH was reported as 0\n",
    "    # price of ETH is unaffected, so using it instead\n",
    "    if '0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2' in query_url:\n",
    "        query_url = query_url.replace('/contract/0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2','')\n",
    "    prices_dict = {'prices': prices_json.get(checksum_token_address, None)}\n",
    "    if prices_dict['prices'] is None:\n",
    "        prices_dict = None\n",
    "    tries = 0\n",
    "    while prices_dict is None:\n",
    "        token_prices = requests.get(query_url)\n",
    "        try:\n",
    "            prices_dict = json.loads(token_prices.content)\n",
    "        except:\n",
    "            pass\n",
    "        # sleep for one second to avoid being blocked\n",
    "        sleep(1)\n",
    "        tries += 1\n",
    "        if tries > 5:\n",
    "            break\n",
    "        whitelist_df.loc[i,'prices_api_response'] = token_prices.content\n",
    "    whitelist_df.loc[i,'prices_dict'] = [prices_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tokens_not_found = whitelist_df[whitelist_df['prices_dict'].apply(lambda x: 'error' in x.keys())].index\n",
    "whitelist_df.drop(index=tokens_not_found, inplace=True)\n",
    "print('Tokens not found in Coingecko: {}'.format(list(tokens_not_found)))\n",
    "\n",
    "whitelist_df['prices_lists'] = whitelist_df['prices_dict'].apply(lambda x: x.get('prices'))\n",
    "prices_not_found = whitelist_df[whitelist_df['prices_lists'].apply(lambda x: len(x)==0)].index\n",
    "whitelist_df.drop(index=prices_not_found, inplace=True)\n",
    "print('Prices not found in Coingecko for: {}'.format(list(prices_not_found)))\n",
    "\n",
    "exploded_whitelist_df = whitelist_df.explode('prices_lists').dropna()\n",
    "exploded_whitelist_df.reset_index(inplace=True)\n",
    "exploded_whitelist_df[['timestamp','price']] = pd.DataFrame(exploded_whitelist_df.prices_lists.tolist(), index=exploded_whitelist_df.index)\n",
    "\n",
    "prices_df = exploded_whitelist_df[['token_address', 'checksum_token_address', 'cap', 'timestamp', 'price']].copy()\n",
    "\n",
    "prices_df['ts_price'] = prices_df.apply(lambda x: [x['timestamp'], x['price']], axis=1)\n",
    "\n",
    "if not REALTIME_ESTIMATOR:\n",
    "    prices_df.groupby('checksum_token_address').agg(list)['ts_price'].to_json(reports_dir+'/_prices.json',\n",
    "                                                                 orient='index',\n",
    "                                                                 indent=4\n",
    "                                                                )\n",
    "\n",
    "prices_df['timestamp'] = prices_df['timestamp']//1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get eligible token balances of every balancer pool at every snapshot block from Big Query\n",
    "get_pools_sql = '''\n",
    "SELECT pool FROM `blockchain-etl.ethereum_balancer.BFactory_event_LOG_NEW_POOL` \n",
    "'''\n",
    "\n",
    "sql = \"\"\"\n",
    "select * from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "where token_address in (\\'{0}\\')\n",
    "and address in ({1})\n",
    "and token_address not in ('0xd46ba6d942050d489dbd938a2c909a5d5039a161')\n",
    "and block_number in ({2})\n",
    "and balance > 0\n",
    "\n",
    "union all\n",
    "\n",
    "select '0xd46ba6d942050d489dbd938a2c909a5d5039a161' as token_address,\n",
    "* from `blockchain-etl.ethereum_balancer.view_token_balances_subset_AMPL`\n",
    "where address in ({1})\n",
    "and block_number in ({2})\n",
    "and balance > 0\n",
    "\n",
    "\"\"\".format('\\',\\''.join(whitelist_df.index), # only get balances of tokens for which there is a price feed\n",
    "           get_pools_sql, \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "pools_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(pools_balances)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_balances['scaled_balance'] = pools_balances['balance'] * pools_balances.join(whitelist_df['decimals'], on='token_address')['decimals'].apply(lambda x: 10**(-x))\n",
    "pools_balances['timestamp'] = pools_balances['block_number'].apply(lambda x: snapshot_blocks_timestamps[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_balances.set_index(['address','block_number','token_address'], inplace=True)\n",
    "number_of_liquid_eligible_tokens = pools_balances.groupby(['address','block_number']).size()\n",
    "number_of_liquid_eligible_tokens.name = 'number_of_liquid_eligible_tokens'\n",
    "pools_balances = pools_balances.join(number_of_liquid_eligible_tokens)\n",
    "eligible_pools_balances = pools_balances[pools_balances['number_of_liquid_eligible_tokens']>=2].copy()\n",
    "eligible_pools_balances.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge balances and prices datasets on nearest timestamp, and compute USD balance of each token in each pool at each block\n",
    "usd_pools_balances = pd.merge_asof(eligible_pools_balances.sort_values(by='timestamp'), \n",
    "                                   prices_df.sort_values(by='timestamp'), \n",
    "                                   on='timestamp', by='token_address', direction='nearest')\n",
    "usd_pools_balances['usd_balance'] = usd_pools_balances['scaled_balance'] * usd_pools_balances['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token weights and swap fees of pools with public swap enabled\n",
    "sql = \"\"\"\n",
    "select W.*, swapfee from `blockchain-etl.ethereum_balancer.view_pools_tokens_denorm_weights` W\n",
    "inner join `blockchain-etl.ethereum_balancer.view_pools_settings_state` S\n",
    "on W.address = S.address and W.block_number = S.block_number\n",
    "where S.public_swap = 'true'\n",
    "and W.denorm > 0\n",
    "and token_address in (\\'{}\\')\n",
    "and W.address in ('{}')\n",
    "and W.block_number in ({})\n",
    "\"\"\".format('\\',\\''.join(whitelist_df.index), # only get weights of tokens for which there is a price feed\n",
    "           '\\',\\''.join(eligible_pools_balances['address'].drop_duplicates()), \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "pools_weights = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(pools_weights)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the merge removes records associated with balances of tokens that are not part of the pool\n",
    "pools_weights_balances = pools_weights.merge(usd_pools_balances, \n",
    "                                             on=['address', 'token_address', 'block_number'],\n",
    "                                             how='inner')\n",
    "pools_weights_balances.set_index(['address', 'token_address', 'block_number'], inplace=True)\n",
    "pools_weights_balances['denorm'] = pools_weights_balances['denorm'].apply(float)\n",
    "summed_weights = pools_weights_balances['denorm'].groupby(['address','block_number']).sum()\n",
    "norm_weights = pools_weights_balances['denorm'] / summed_weights\n",
    "norm_weights.name = 'norm_weights'\n",
    "pools_weights_balances = pools_weights_balances.join(norm_weights)\n",
    "# remove pools with only one eligible token\n",
    "pools_weights_balances = pools_weights_balances[pools_weights_balances['norm_weights']<1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Computing wrap factor...')\n",
    "wrap_factor = pools_weights_balances['norm_weights'].\\\n",
    "                    groupby(['address','block_number']).\\\n",
    "                    agg(get_wrap_factor)\n",
    "wrap_factor.name = 'wrap_factor'\n",
    "pools_weights_balances = pools_weights_balances.join(wrap_factor)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Computing BRF (first pass)...')\n",
    "brf = pools_weights_balances['norm_weights']. \\\n",
    "                groupby(['address','block_number']). \\\n",
    "                agg(get_BRF)\n",
    "brf.name = 'first_pass_brf'\n",
    "pools_weights_balances = pools_weights_balances.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the fee factor\n",
    "# https://forum.balancer.finance/t/modifying-feefactor-toward-reducing-the-mining-penalty-for-high-fee-pools/103\n",
    "# a swapfee of 1% is stored in the smart contracts as 1e+16 (0.01e+18)\n",
    "# fee factor formula as defined in the specs above takes as argument the fee as a percentage (eg 1 for a fee of 1% - not 0.01)\n",
    "pools_weights_balances['fee_factor'] = np.exp(-(0.25 * \\\n",
    "                                                (100 * \\\n",
    "                                                 (pools_weights_balances['swapfee'].astype(float) / 1E18)))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_weights_balances['adjustedLiquidityPreTokenCap'] = pools_weights_balances['usd_balance'] * \\\n",
    "                                                            pools_weights_balances['fee_factor'] * \\\n",
    "                                                            pools_weights_balances['wrap_factor'] * \\\n",
    "                                                            pools_weights_balances['first_pass_brf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the tokenCapFactor for each token_address at each block_number\n",
    "tokenCapFactor = np.minimum(1, whitelist_df['cap'] / (pools_weights_balances['adjustedLiquidityPreTokenCap'].\\\n",
    "    groupby(['block_number','token_address']).\\\n",
    "    sum().\\\n",
    "    sort_values()))\n",
    "tokenCapFactor.name = 'tokenCapFactor'\n",
    "pools_weights_balances = pools_weights_balances.join(tokenCapFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools_weights_balances['token_capped_usd_balance'] = pools_weights_balances['usd_balance'] * \\\n",
    "                                                        pools_weights_balances['tokenCapFactor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get liquidity providers and the amount of BPT each has\n",
    "# private and smart pools don't have BPT, so we assign 1 fictitious BPT to the controller\n",
    "# BAL mined by controllers of smart pools created by the CRPFactory will be redistributed to the controller token holders later in the process\n",
    "sql = \"\"\"\n",
    "with shared_pools as (\n",
    "  select token_address as address, address as bpt_holder, block_number, balance as bpt from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "  where token_address in ('{0}')\n",
    "  and block_number in ({1})\n",
    "  and balance > 0\n",
    "),\n",
    "private_pools as (\n",
    "  select address, controller as bpt_holder, block_number, 1 as bpt from `blockchain-etl.ethereum_balancer.view_pools_controllers`\n",
    "  where address not in (select address from shared_pools)\n",
    "  and block_number in ({1})\n",
    ")\n",
    "select * from shared_pools\n",
    "union all\n",
    "select * from private_pools\n",
    "\"\"\".format('\\',\\''.join(pools_weights_balances.index.get_level_values('address').drop_duplicates()), \n",
    "           ','.join(snapshot_blocks_as_str))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import time\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying BigQuery...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "bpt_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "n = len(bpt_balances)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Done ({n} records)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_shareholder = bpt_balances['bpt_holder'].isin(BLACKLISTED_SHAREHOLDERS_lower)\n",
    "bpt_balances['is_shareholder'] = is_shareholder\n",
    "bpt_balances.set_index(['address','block_number','is_shareholder','bpt_holder'], inplace=True)\n",
    "bpt_balances.rename_axis(index={'is_shareholder': 'shareholders_subpool'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split pools that have a blacklisted shareholder as one of their LPs\n",
    "split_pools = bpt_balances['bpt'].groupby(['address','block_number','shareholders_subpool']).sum()\n",
    "total_bpt = bpt_balances['bpt'].groupby(['address','block_number']).sum()\n",
    "relative_size_of_subpool = split_pools/total_bpt\n",
    "relative_size_of_subpool.name = 'relative_size_of_subpool'\n",
    "subpools = pools_weights_balances.join(relative_size_of_subpool, how='inner')\n",
    "\n",
    "# recompute values according to the relative size of the subpool\n",
    "splitable_cols = ['balance', 'scaled_balance', 'usd_balance', 'adjustedLiquidityPreTokenCap', \n",
    "                'token_capped_usd_balance']\n",
    "for c in splitable_cols:\n",
    "    subpools[c] = subpools[c] * subpools['relative_size_of_subpool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_BAL_MULTIPLIER = 3\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Second BRF - no BAL multiplier...')\n",
    "\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF)\n",
    "brf.name = 'second_pass_brf_mult1'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + f' - Second BRF - with temp BAL multiplier ({TEMP_BAL_MULTIPLIER})...')\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF, bal_multiplier = TEMP_BAL_MULTIPLIER)\n",
    "brf.name = 'second_pass_brf_with_temp_mult'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpools['adjustedLiquidityPreStaking'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['second_pass_brf_mult1']\n",
    "\n",
    "subpools['adjustedLiquidityWithTempStakingMult'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['second_pass_brf_with_temp_mult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute final BAL multiplier\n",
    "total_adjustedLiquidityPreStaking = subpools['adjustedLiquidityPreStaking'].groupby('block_number').sum()\n",
    "total_adjustedLiquidityWithStakingTempMult = subpools['adjustedLiquidityWithTempStakingMult'].groupby('block_number').sum()\n",
    "final_desired_adjusted_liquidity = total_adjustedLiquidityPreStaking / (1-STAKERS_SHARE)\n",
    "stretch = (final_desired_adjusted_liquidity - total_adjustedLiquidityPreStaking) / \\\n",
    "            (total_adjustedLiquidityWithStakingTempMult - total_adjustedLiquidityPreStaking)\n",
    "final_bal_multiplier = 1 + stretch * (TEMP_BAL_MULTIPLIER - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Third BRF - with final BAL multiplier...')\n",
    "brf = subpools['norm_weights']. \\\n",
    "                groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                agg(get_BRF, bal_multiplier = final_bal_multiplier)\n",
    "brf.name = 'third_pass_brf_with_final_mult'\n",
    "subpools = subpools.join(brf)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the final adjusted liquidity of each token in each subpool at each block\n",
    "subpools['finalAdjustedLiquidity'] = subpools['token_capped_usd_balance'] * \\\n",
    "                                            subpools['fee_factor'] * \\\n",
    "                                            subpools['wrap_factor'] * \\\n",
    "                                            subpools['third_pass_brf_with_final_mult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the total final adjusted liquidity at each block\n",
    "total_final_adjustedLiquidity = subpools['finalAdjustedLiquidity'].groupby('block_number').sum()\n",
    "\n",
    "# compute the share of liquidity provided by each token in each subpool\n",
    "share_of_liquidity = subpools['finalAdjustedLiquidity'] / total_final_adjustedLiquidity\n",
    "share_of_liquidity.name = 'share_of_liquidity'\n",
    "subpools = subpools.join(share_of_liquidity)\n",
    "\n",
    "# compute the BAL mined by each token in each subpool at each block, proportional to the share of liquidity\n",
    "subpools['BAL_mined'] = subpools['share_of_liquidity'] * WEEKLY_MINED / len(snapshot_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the BAL mined by each LP proportional to their share of the pool\n",
    "bal_mined_by_subpool_per_block = subpools['BAL_mined']. \\\n",
    "                                    groupby(['address','block_number','shareholders_subpool']). \\\n",
    "                                    sum()\n",
    "\n",
    "total_bpt = bpt_balances['bpt'].groupby(['address','block_number','shareholders_subpool']).sum()\n",
    "share_of_pool = bpt_balances['bpt'] / total_bpt\n",
    "\n",
    "bal_mined = bpt_balances.copy()\n",
    "bal_mined['bal_mined'] = (bal_mined_by_subpool_per_block * share_of_pool)\n",
    "bal_mined.reset_index(inplace=True)\n",
    "chksums = {x: Web3.toChecksumAddress(x) for x in bal_mined['bpt_holder'].drop_duplicates()}\n",
    "bal_mined['chksum_bpt_holder'] = bal_mined['bpt_holder'].apply(lambda x: chksums[x])\n",
    "bal_mined.set_index(['address', 'block_number', 'shareholders_subpool', 'chksum_bpt_holder'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = bal_mined['bal_mined'].groupby('chksum_bpt_holder').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals[totals>=1e-18].apply(lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totalsPreRedirect.json',\n",
    "                                                  indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpools['datetime'] = pd.to_datetime(subpools.timestamp, unit='s')\n",
    "rewards_per_token = subpools.groupby(['token_address','datetime']).sum()['BAL_mined']\n",
    "top_tokens = subpools['BAL_mined'].groupby(['token_address']).sum().sort_values(ascending=False).head(10).index\n",
    "ax = pd.DataFrame(rewards_per_token).reset_index().\\\n",
    "    pivot(index='datetime', columns='token_address', values='BAL_mined')[top_tokens].\\\n",
    "    plot(figsize = (15,10),\n",
    "         title = 'BAL mined by top 10 tokens')\n",
    "print('Top 10 tokens:\\n' + '\\n'.join(top_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_pool = subpools.groupby(['address','datetime']).sum()['BAL_mined']\n",
    "top_pools = subpools['BAL_mined'].groupby(['address']).sum().sort_values(ascending=False).head(10).index\n",
    "ax = pd.DataFrame(rewards_per_pool).reset_index().\\\n",
    "    pivot(index='datetime', columns='address', values='BAL_mined')[top_pools].\\\n",
    "    plot(figsize = (15,10),\n",
    "         title = 'BAL earned by top 10 pools')\n",
    "print('Top 10 pools:\\n' + '\\n'.join(top_pools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_lp = bal_mined['bal_mined'].groupby(['chksum_bpt_holder','block_number']).sum()\n",
    "top_lps = bal_mined['bal_mined'].groupby(['chksum_bpt_holder']).sum().sort_values(ascending=False).head(10).index\n",
    "ax = pd.DataFrame(rewards_per_lp).reset_index().\\\n",
    "    pivot(index='block_number', columns='chksum_bpt_holder', values='bal_mined')[top_lps].\\\n",
    "    plot(figsize = (15,10),\n",
    "         title = 'BAL earned by top 10 liquidity providers')\n",
    "ax.ticklabel_format(axis='x', style='plain')\n",
    "print('Top 10 LPs:\\n' + '\\n'.join(top_lps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(15, 10))\n",
    "\n",
    "i = 0\n",
    "areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "            reset_index().pivot(index='datetime', columns='address', values='usd_balance')\n",
    "# deterministically color code the regions of the plot for visual inspection between weeks\n",
    "areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "areaplot.plot.area(legend=False, ax=axs.flat[i], title='USD balance by pool', colormap=pal)\n",
    "\n",
    "i += 1\n",
    "areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "            reset_index().pivot(index='datetime', columns='address', values='finalAdjustedLiquidity')\n",
    "areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "areaplot.plot.area(legend=False, ax=axs.flat[i], title='Adjusted liquidity by pool', colormap=pal)\n",
    "\n",
    "i += 1\n",
    "areaplot = subpools.groupby(['datetime','address']).sum().\\\n",
    "            reset_index().pivot(index='datetime', columns='address', values='BAL_mined')\n",
    "areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "areaplot.plot.area(legend=False, ax=axs.flat[i], title='BAL mined by pool', colormap=pal)\n",
    "\n",
    "i += 1\n",
    "areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "            reset_index().pivot(index='datetime', columns='token_address', values='usd_balance')\n",
    "# deterministically color code the regions of the plot for visual inspection between weeks\n",
    "areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "areaplot.plot.area(legend=False, ax=axs.flat[i], title='USD balance by token', colormap=pal)\n",
    "\n",
    "i += 1\n",
    "areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "            reset_index().pivot(index='datetime', columns='token_address', values='finalAdjustedLiquidity')\n",
    "areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "areaplot.plot.area(legend=False, ax=axs.flat[i], title='Adjusted liquidity by token', colormap=pal)\n",
    "\n",
    "i += 1\n",
    "areaplot = subpools.groupby(['datetime','token_address']).sum().\\\n",
    "            reset_index().pivot(index='datetime', columns='token_address', values='BAL_mined')\n",
    "areaplot = areaplot[list(areaplot.sum().sort_values(ascending=False).index)]\n",
    "pal = ListedColormap(list(map(lambda x: '#'+x[20:26], list(areaplot.columns))))\n",
    "areaplot.plot.area(legend=False, ax=axs.flat[i], title='BAL mined by token', colormap=pal)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redirect\n",
    "This redirects BAL earned by one address to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redirect.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redirects = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redirects = json.load(open('config/redirect.json'))\n",
    "redirection = bal_mined.reset_index()\n",
    "redirection['redirect_to'] = redirection['chksum_bpt_holder'].apply(lambda x: redirects.get(x,x))\n",
    "redirected_totals = redirection.set_index('redirect_to')['bal_mined'].groupby('redirect_to').sum()\n",
    "redirected_totals[redirected_totals>=1e-18].apply(\\\n",
    "   lambda x: format(x, f'.{CLAIM_PRECISION}f')).to_json(reports_dir+'/_totalsPreRedistribute.json',\n",
    "   indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redistribute\n",
    "This redistributes BAL earned by a controller to the token holders of that controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get receivers to redistribute\n",
    "if REALTIME_ESTIMATOR:\n",
    "    url = 'https://raw.githubusercontent.com/balancer-labs/bal-mining-scripts/master/config/redistribute.json'\n",
    "    jsonurl = urlopen(url)\n",
    "    redistributers_dict = json.loads(jsonurl.read())\n",
    "else:\n",
    "    redistributers_dict = json.load(open('config/redistribute.json'))\n",
    "redistributers_list = list(redistributers_dict.keys())\n",
    "# get list of CRPs\n",
    "sql = 'SELECT pool FROM `blockchain-etl.ethereum_balancer.CRPFactory_event_LogNewCrp`'\n",
    "# Requires setting the environment variable GOOGLE_APPLICATION_CREDENTIALS \n",
    "# to the file path of the JSON file that contains a service account key \n",
    "# with access to the token_balances_subset view\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "crps = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "\n",
    "redistributers_list.extend(crps['pool'].drop_duplicates().apply(Web3.toChecksumAddress))\n",
    "print('Redistributers: {}'.format(redistributers_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_to_redistribute_df = bal_mined.reset_index()[['chksum_bpt_holder','block_number','bal_mined']]\n",
    "rewards_to_redistribute_df.dropna(inplace=True)\n",
    "mask = rewards_to_redistribute_df['chksum_bpt_holder'].isin(redistributers_list)\n",
    "rewards_to_redistribute_df = rewards_to_redistribute_df[mask].reset_index(drop=True)\n",
    "rewards_to_redistribute_df['redistributer'] = rewards_to_redistribute_df['chksum_bpt_holder'].apply(lambda x: x.lower())\n",
    "rewards_to_redistribute_df['value'] = rewards_to_redistribute_df['bal_mined'].astype(float)\n",
    "rewards_to_redistribute_df = rewards_to_redistribute_df[['redistributer','block_number','value']]\n",
    "# rewards_to_redistribute_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get redistributers' token holders\n",
    "tokens = list(rewards_to_redistribute_df.redistributer.drop_duplicates())\n",
    "snapshot_block_numbers = list(rewards_to_redistribute_df.block_number.drop_duplicates())\n",
    "snapshot_block_numbers = [str(b) for b in snapshot_block_numbers]\n",
    "sql = \"\"\"\n",
    "select * from `blockchain-etl.ethereum_balancer.view_token_balances_subset`\n",
    "where token_address in ({})\n",
    "and block_number in ({})\n",
    "and balance <> 0\n",
    "\"\"\".format('\\''+'\\',\\''.join(tokens)+'\\'', \n",
    "           ','.join(snapshot_block_numbers))\n",
    "# print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires setting the environment variable GOOGLE_APPLICATION_CREDENTIALS \n",
    "# to the file path of the JSON file that contains a service account key \n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Querying Bigquery for the token holders of the controllers ...')\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "running_balances = (\n",
    "    client.query(sql)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "running_balances['balance'] = running_balances['balance'].astype(float)\n",
    "running_balances = running_balances.rename(columns={\"token_address\": \"redistributer\", \"address\": \"share_holder\"})\n",
    "running_balances.set_index(['block_number','redistributer','share_holder'], inplace=True)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' - Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares = pd.DataFrame(running_balances['balance'])/pd.DataFrame(running_balances.groupby(['block_number','redistributer']).sum()['balance'])\n",
    "shares.columns = ['perc_share']\n",
    "rewards_to_redistribute_df.set_index(['block_number', 'redistributer'], inplace=True)\n",
    "rewards_to_redistribute_df.fillna(0, inplace=True)\n",
    "rewards_to_redistribute_df.head()\n",
    "redistribution_df = pd.DataFrame(rewards_to_redistribute_df['value']*shares['perc_share']).dropna().reset_index()\n",
    "redistribution_df.rename(columns={0: 'reward'}, inplace=True)\n",
    "redistribution_df['share_holder'] = redistribution_df['share_holder'].apply(Web3.toChecksumAddress)\n",
    "redistribution_df['redistributer'] = redistribution_df['redistributer'].apply(Web3.toChecksumAddress)\n",
    "redistribution_df.loc[redistribution_df['reward']<0,'reward'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Uncomment to plot redistributions\n",
    "# actual_redistributers = list(redistribution_df.redistributer.drop_duplicates())\n",
    "# fig, axs = plt.subplots(ncols=2, nrows=len(actual_redistributers), figsize=(15, 5*len(actual_redistributers)))\n",
    "# for i,t in enumerate(actual_redistributers):\n",
    "#     areaplot = redistribution_df[redistribution_df.redistributer==t].pivot(index='block_number', \n",
    "#                                                                            columns='share_holder', \n",
    "#                                                                            values='reward')\n",
    "#     areaplot.plot.area(legend=False, ax=axs.flat[2*i],\n",
    "#                        title='Redistributed rewards and holders ({})'.format(t[:8]));\n",
    "#     areaplot.divide(areaplot.sum(axis=1), axis=0).plot.area(legend=False, ax=axs.flat[2*i+1],\n",
    "#                                                             title='Share of redistributed rewards and holders ({})'.format(t[:8]));\n",
    "#     axs.flat[2*i].ticklabel_format(axis='x', style='plain')\n",
    "#     axs.flat[2*i+1].ticklabel_format(axis='x', style='plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_redistribution = redistribution_df.groupby('share_holder').sum()['reward']\n",
    "total_pre_redistribution = json.load(open(reports_dir+'/_totalsPreRedistribute.json'))\n",
    "total_post_redistribution = total_pre_redistribution.copy()\n",
    "for index,value in total_redistribution.iteritems():\n",
    "    old_value = float(total_pre_redistribution.get(index,0))\n",
    "    new_value = old_value + value\n",
    "    total_post_redistribution[index] = format(new_value, f'.{CLAIM_PRECISION}f')\n",
    "\n",
    "for r in redistribution_df['redistributer'].drop_duplicates():\n",
    "    del total_post_redistribution[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BAL  pre-redistribution: {:.18f}'.format(sum([float(v) for v in total_pre_redistribution.values()])))\n",
    "print('BAL post-redistribution: {:.18f}'.format(sum([float(v) for v in total_post_redistribution.values()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a final redirect to handle contracts that hold smart pools tokens and can't withdraw BAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects = json.load(open('config/redirect.json'))\n",
    "for src,dst in redirects.items():\n",
    "    if src in total_post_redistribution.keys():\n",
    "        if dst not in total_post_redistribution.keys():\n",
    "            total_post_redistribution[dst] = 0\n",
    "        total_post_redistribution[dst] = float(total_post_redistribution[dst])\n",
    "        total_post_redistribution[dst] += float(total_post_redistribution.pop(src))\n",
    "        total_post_redistribution[dst] = format(total_post_redistribution[dst], \n",
    "                                                f'.{CLAIM_PRECISION}f')\n",
    "print('BAL post-redistribution and final redirect: {:.18f}'.format(sum([float(v) for v in total_post_redistribution.values()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(total_post_redistribution, \n",
    "          open(reports_dir+'/_totals.json', mode='w'),\n",
    "          indent=4,\n",
    "          separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to GBQ\n",
    "if REALTIME_ESTIMATOR:\n",
    "    cur_estimate = pd.read_json(f'reports/{WEEK}/_totals.json', orient='index', dtype=False)\n",
    "    cur_estimate.columns = ['earned']\n",
    "    cur_estimate.index.name = 'address'\n",
    "    \n",
    "    try:\n",
    "        prev_estimate = pd.read_gbq('select address, earned, timestamp from bal_mining_estimates.wallet_estimates')\n",
    "        prev_estimate.set_index('address', inplace=True)\n",
    "        prev_estimate_timestamp = prev_estimate.loc[0, 'timestamp']\n",
    "    except:\n",
    "        prev_estimate_timestamp = 0\n",
    "    if prev_estimate_timestamp < start_block_timestamp:\n",
    "        #previous estimate is last week's; compute velocity from end_block_timestamp and start_block_timestamp\n",
    "        delta_t = (end_block_timestamp - start_block_timestamp)\n",
    "        earned = cur_estimate['earned'].astype(float)\n",
    "        cur_estimate['velocity'] = (earned/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "    else:\n",
    "        #compute velocity based on increase and time passed\n",
    "        delta_t = (end_block_timestamp - prev_estimate_timestamp)\n",
    "        diff_estimate = cur_estimate.join(prev_estimate, rsuffix='_prev').fillna(0)\n",
    "        cur_earned = diff_estimate['earned'].astype(float)\n",
    "        prev_earned = diff_estimate['earned_prev'].astype(float)\n",
    "        cur_estimate['velocity'] = ((cur_earned-prev_earned)/delta_t).apply(lambda x: format(x, f'.{18}f'))\n",
    "        \n",
    "    cur_estimate['timestamp'] = end_block_timestamp\n",
    "    cur_estimate.reset_index(inplace=True)\n",
    "    cur_estimate.to_gbq( 'bal_mining_estimates.wallet_estimates', if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
